{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras models\n",
    "This is based on the Keras tutorial (https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) and Keras Convolutional Model notebook (https://github.com/JoeDumoulin/CSCD439F17/blob/master/notebooks/Final%20Project/Keras%20Convolutional%20Network%20for%20Spooky%20Author%20ID1.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17600307339371531092\n",
      "]\n",
      "[[ 22.  28.]\n",
      " [ 49.  64.]]\n"
     ]
    }
   ],
   "source": [
    "# Definitions\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# tensorflow settings to activate gpu\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "\n",
    "BASE_DIR = '../data'\n",
    "GLOVE_DIR = 'glove.6B'\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, 'SpookyData')\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "import tensorflow as tf\n",
    "# Creates a graph.\n",
    "a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the training data\n",
    "df = pd.read_csv('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MWS': 2, 'EAP': 0, 'HPL': 1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>author_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "   author_id  \n",
       "0          0  \n",
       "1          1  \n",
       "2          0  \n",
       "3          2  \n",
       "4          1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a list of classifications and generate numeric \n",
    "#  values for each class.  put the numeric class back \n",
    "#  on to the data frame.\n",
    "authors = dict([(auth, idx) for idx, auth in enumerate(df['author'].unique())])\n",
    "print(authors)\n",
    "df['author_id'] = df['author'].apply(lambda x: authors[x])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19579\n",
      "[26, 2945, 143, 1372, 22, 36, 294, 2, 7451, 1, 2440, 2, 10, 4556, 16, 6, 79, 179, 48, 4245, 3, 295, 4, 1, 249, 1943, 6, 326, 74, 134, 123, 891, 2, 1, 313, 39, 1438, 4928, 98, 1, 430]\n",
      "Found 25943 unique tokens before stopwords removal.\n",
      "[('splendour', 2558), ('waddle', 15025), ('atlantic', 4141), ('foraging', 16866), ('millstone', 16530)]\n",
      "Found 25808 unique tokens after stopwords removal.\n",
      "Shape of data tensor: (19579, 1000)\n",
      "Shape of label tensor: (19579, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop stop words. These common words probably won't provide insight into which author wrote each sentence\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "# now we will use the text and author_id fields to train a classifier.\n",
    "#  We have to: \n",
    "#  1. Get the sentences, \n",
    "# this takes each sentence from the training file and places it in the list\n",
    "sents = df['text'].tolist()\n",
    "# this takes each author id (assigned above) and places them in a list,\n",
    "# so that we can compare our results to the actual authors\n",
    "labels = df['author_id'].tolist()\n",
    "\n",
    "#  2. Tokenize each sentence, \n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(sents)\n",
    "# turns the text input into numerical arrays\n",
    "sequences = tokenizer.texts_to_sequences(sents)\n",
    "print(len(sequences))\n",
    "print(sequences[0])\n",
    "##    Get a vector of unique terms here\n",
    "print('Found %s unique tokens before stopwords removal.' % len(tokenizer.word_index))\n",
    "print([w for w in tokenizer.word_index.items()][:5])\n",
    "word_index = dict([(w,i) for w,i in tokenizer.word_index.items() if w not in stops])\n",
    "print('Found %s unique tokens after stopwords removal.' % len(word_index))\n",
    "\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "#shuffling indices takes less time than shuffling objects\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "# sets the number of the validation samples to 20% of the data (20% is the percentage selected above)\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "y_val[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cells are used in both the first and second models, which is why I left it outside of the \"Model 1\" section. The data is read in from the \"train.csv\" file and prepared for use in the models. The models below all use embeddings; some use preprepared GloVe embeddings, and some train embeddings based on the training set.\n",
    "\n",
    "GloVe (Global Vectors for Word Representation) is an \"unsupervised learning algorithm for obtaining vector representations of words,\" developed at Stanford (https://nlp.stanford.edu/projects/glove/). The GloVe model is trained on non-zero entries in a word-word co-occurence matrix. Developing the matrix requires a one-time pass over the corpus, which can be expensive up front but saves time in the long-run. In the matrix, words which are closely associated with each other appear closer together than words which rarely occur near each other in the corpus.\n",
    "\n",
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#  3. Load embeddings\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.300d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses ones of several GloVe text files. Using one of the other files may be more effective. The length of the word vectors in the files used is 300. The other options are 50, 100, and 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2092\n"
     ]
    }
   ],
   "source": [
    "#  4. Create the Embedding matrix for the training set\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "# returns an array of the size num_words x EMBEDDING_DIM, filled with 0s\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "unk = []\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    # gets the vector for the current word    \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        unk.append(word)\n",
    "print(len(unk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted in the comments in the cell above, this is specifically structured so that words not found in the embedding index will be all-zeros. Considering HP Lovecraft makes up many words, this might cause some issues if his imaginary words were not in the data sampled for the GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "print('Training model.')\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(.5)(x)\n",
    "preds = Dense(len(authors), activation='softmax')(x)\n",
    "rms = RMSprop(lr=0.003)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='mean_squared_logarithmic_error',\n",
    "              optimizer=rms, #'rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/2\n",
      "15664/15664 [==============================] - 1113s 71ms/step - loss: 0.0616 - acc: 0.7293 - val_loss: 0.0542 - val_acc: 0.7535\n",
      "Epoch 2/2\n",
      "15664/15664 [==============================] - 1091s 70ms/step - loss: 0.0323 - acc: 0.8678 - val_loss: 0.0555 - val_acc: 0.7757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdd5bcc7048>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=100,\n",
    "          epochs=2,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 300)         6000000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 996, 128)          192128    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 6,209,027\n",
      "Trainable params: 6,209,027\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this model has one input layer, one embedding layer, one convolutional layer, one pooling layer, two dense layers, and one dropout layer.\n",
    "\n",
    "After two epochs, the validation accuracy of this model was 77.57%. Running more epochs could result in higher accuracy, but due to the amount of time it takes to run each model, I will be testing various models with two epochs, and then run the best model for more epochs in order to prepare the final submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 -- Not pretrained embeddings\n",
    "As I noted above, I was concerned that using the pretrained embeddings might cause issues due to Lovecraft's tendency to make up words. These imaginary words (Cthulhu, R'lyeh, etc.) are highly indicative of Lovecraft's writing and could affect the accuracy of the model. In this model, the embedding layer is trained based off the word_index vector defined above. In all other respects, the model is identical to Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n"
     ]
    }
   ],
   "source": [
    "# train word embeddings and load into an Embedding layer\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Training model.')\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(.5)(x)\n",
    "preds = Dense(len(authors), activation='softmax')(x)\n",
    "rms = RMSprop(lr=0.003)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='mean_squared_logarithmic_error',\n",
    "              optimizer=rms, #'rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/2\n",
      "15664/15664 [==============================] - 1074s 69ms/step - loss: 0.0698 - acc: 0.6802 - val_loss: 0.0500 - val_acc: 0.7834\n",
      "Epoch 2/2\n",
      "15664/15664 [==============================] - 1051s 67ms/step - loss: 0.0298 - acc: 0.8759 - val_loss: 0.0555 - val_acc: 0.7732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f56a74f4470>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=100,\n",
    "          epochs=2,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model was very slightly less accurate after the second epoch (77.32% vs 77.57%), but interestingly, it was more accurate after the first epoch (78.34%, compared to 75.35% after the first epoch in Model 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3 -- Pretrained embeddings of length 100 (instead of 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#  3. Load embeddings\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2092\n"
     ]
    }
   ],
   "source": [
    "#  4. Create the Embedding matrix for the training set\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "# returns an array of the size num_words x EMBEDDING_DIM, filled with 0s\n",
    "embedding_matrix = np.zeros((num_words, 100))\n",
    "unk = []\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    # gets the vector for the current word    \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        unk.append(word)\n",
    "print(len(unk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            100,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "print('Training model.')\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(.5)(x)\n",
    "preds = Dense(len(authors), activation='softmax')(x)\n",
    "rms = RMSprop(lr=0.003)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='mean_squared_logarithmic_error',\n",
    "              optimizer=rms, #'rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/2\n",
      "15664/15664 [==============================] - 566s 36ms/step - loss: 0.0838 - acc: 0.6108 - val_loss: 0.0581 - val_acc: 0.7425\n",
      "Epoch 2/2\n",
      "15664/15664 [==============================] - 585s 37ms/step - loss: 0.0480 - acc: 0.7956 - val_loss: 0.0752 - val_acc: 0.6861\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f56a22d61d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=100,\n",
    "          epochs=2,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is almost ten percentage points worse than model 1. When using pretrained embeddings, it appears sticking with the longer word vectors is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4 -- Not pretrained embeddings, sentences to lower\n",
    "I tried to find whether the capitalization of the words being used to train the embedding mattered, but nothing seemed to come up when I googled it. In most circumstances, capital letters are treated as entirely different than lowercase letters, so I tried using a non-pretrained embedding layer again, this time setting all the words in the sentences to lowercase beforehand. Changing the capitalization is the only difference between this model and model 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19579\n",
      "[26, 2945, 143, 1372, 22, 36, 294, 2, 7451, 1, 2440, 2, 10, 4556, 16, 6, 79, 179, 48, 4245, 3, 295, 4, 1, 249, 1943, 6, 326, 74, 134, 123, 891, 2, 1, 313, 39, 1438, 4928, 98, 1, 430]\n",
      "Found 25943 unique tokens before stopwords removal.\n",
      "[('city', 224), ('oozing', 24131), ('unsatisfying', 10739), ('aye', 6915), ('dooms', 15393)]\n",
      "Found 25808 unique tokens after stopwords removal.\n",
      "Shape of data tensor: (19579, 1000)\n",
      "Shape of label tensor: (19579, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop stop words\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "# now we will use the text and author_id fields to train a classifier.\n",
    "#  We have to: \n",
    "#  1. Get the sentences, \n",
    "sents = df['text'].tolist()\n",
    "labels = df['author_id'].tolist()\n",
    "\n",
    "#make all sentences lower case\n",
    "[x.lower() for x in sents]\n",
    "\n",
    "#  2. Tokenize each sentence, \n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(sents)\n",
    "sequences = tokenizer.texts_to_sequences(sents)\n",
    "print(len(sequences))\n",
    "print(sequences[0])\n",
    "##    Get a vector of unique terms here\n",
    "print('Found %s unique tokens before stopwords removal.' % len(tokenizer.word_index))\n",
    "print([w for w in tokenizer.word_index.items()][:5])\n",
    "word_index = dict([(w,i) for w,i in tokenizer.word_index.items() if w not in stops])\n",
    "print('Found %s unique tokens after stopwords removal.' % len(word_index))\n",
    "\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "y_val[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n"
     ]
    }
   ],
   "source": [
    "# train word embeddings and load into an Embedding layer\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Training model.')\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(.5)(x)\n",
    "preds = Dense(len(authors), activation='softmax')(x)\n",
    "rms = RMSprop(lr=0.003)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='mean_squared_logarithmic_error',\n",
    "              optimizer=rms, #'rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/2\n",
      "15664/15664 [==============================] - 1055s 67ms/step - loss: 0.0703 - acc: 0.6740 - val_loss: 0.0550 - val_acc: 0.7561\n",
      "Epoch 2/2\n",
      "15664/15664 [==============================] - 1029s 66ms/step - loss: 0.0299 - acc: 0.8786 - val_loss: 0.0455 - val_acc: 0.8087\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f56a241ce48>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=100,\n",
    "          epochs=2,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the highest accuracy model yet (80.87% accuracy after two epochs, 3.3 percentage points higher than the second-most accurate model), so it appears that making the sentences entirely lowercase before training the embedding layer helped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5 -- Pretrained embeddings, sentences to lower\n",
    "In this model, I am checking if making the sentences all lowercase improves the accuracy if using a pretrained embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#  3. Load embeddings\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.300d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2092\n"
     ]
    }
   ],
   "source": [
    "#  4. Create the Embedding matrix for the training set\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "unk = []\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        unk.append(word)\n",
    "print(len(unk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "print('Training model.')\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "#x = MaxPooling1D()(x)\n",
    "#x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(.5)(x)\n",
    "preds = Dense(len(authors), activation='softmax')(x)\n",
    "rms = RMSprop(lr=0.003)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='mean_squared_logarithmic_error',\n",
    "              optimizer=rms, #'rmsprop',\n",
    "              metrics=['acc'])\n",
    "#model.compile(loss='categorical_crossentropy',\n",
    "#              optimizer=rms, #'rmsprop',\n",
    "#              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/2\n",
      "15664/15664 [==============================] - 1082s 69ms/step - loss: 0.1006 - acc: 0.5667 - val_loss: 0.0945 - val_acc: 0.6416\n",
      "Epoch 2/2\n",
      "15664/15664 [==============================] - 1053s 67ms/step - loss: 0.0832 - acc: 0.7117 - val_loss: 0.0792 - val_acc: 0.7387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f56a2135400>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=100,\n",
    "          epochs=2,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has the lowest accuracy so far (73.87%). It appears that making all the sentences lowercase only helps when not using pretrained embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 6 - Based on model 4, messing with layers\n",
    "Model 4 was the best model so far, so I will use it as the base for further experimentation This model has another convolutional layer and a max pooling layer which was not in model 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n"
     ]
    }
   ],
   "source": [
    "# train word embeddings and load into an Embedding layer\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Training model.')\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D()(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(.5)(x)\n",
    "preds = Dense(len(authors), activation='softmax')(x)\n",
    "rms = RMSprop(lr=0.003)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='mean_squared_logarithmic_error',\n",
    "              optimizer=rms, #'rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/2\n",
      "15664/15664 [==============================] - 1459s 93ms/step - loss: 0.0737 - acc: 0.6576 - val_loss: 0.0529 - val_acc: 0.7727\n",
      "Epoch 2/2\n",
      "15664/15664 [==============================] - 1844s 118ms/step - loss: 0.0347 - acc: 0.8567 - val_loss: 0.0447 - val_acc: 0.8115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5696d2b240>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=100,\n",
    "          epochs=2,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extra layers made this model slightly more accurate (81.15% after two epochs vs 80.87%), although it took nearly twice as long to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 7 -- Based on Model 4, various tweaks to hyperparameters\n",
    "In this model, I halved the number of filters in the convolutional layer, reduced the kernel size from 5 to 3, and increased the learning rate to .005 from .003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n"
     ]
    }
   ],
   "source": [
    "# train word embeddings and load into an Embedding layer\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Training model.')\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(64, 3, activation='relu')(embedded_sequences)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(.5)(x)\n",
    "preds = Dense(len(authors), activation='softmax')(x)\n",
    "rms = RMSprop(lr=0.005)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='mean_squared_logarithmic_error',\n",
    "              optimizer=rms, #'rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/2\n",
      "15664/15664 [==============================] - 550s 35ms/step - loss: 0.0732 - acc: 0.6616 - val_loss: 0.0607 - val_acc: 0.7390\n",
      "Epoch 2/2\n",
      "15664/15664 [==============================] - 582s 37ms/step - loss: 0.0361 - acc: 0.8498 - val_loss: 0.0511 - val_acc: 0.7819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f56a7684908>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=100,\n",
    "          epochs=2,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model, with an accuracy of 78.19%, is better than the first three models but a step down from Model 4, which it is based off of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Submission\n",
    "This model isn't great, but I made a test submission at this point just to make sure I could get it to work. The loss rate was 2.68128, which isn't particularly surprising given the general state of the model. It's worth noting that the loss rate in the contest score doesn't seem to have much to do with the loss calculated during the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 8 -- Based on Model 4, different loss function, smaller batches, lower dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n"
     ]
    }
   ],
   "source": [
    "# train word embeddings and load into an Embedding layer\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Training model.')\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(.4)(x)\n",
    "preds = Dense(len(authors), activation='softmax')(x)\n",
    "rms = RMSprop(lr=0.003)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='mean_absolute_error',\n",
    "              optimizer=rms, #'rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/2\n",
      "15664/15664 [==============================] - 1528s 98ms/step - loss: 0.3997 - acc: 0.4007 - val_loss: 0.3917 - val_acc: 0.4125\n",
      "Epoch 2/2\n",
      "15664/15664 [==============================] - 1187s 76ms/step - loss: 0.3992 - acc: 0.4012 - val_loss: 0.3917 - val_acc: 0.4125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f56a23b5c18>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=50,\n",
    "          epochs=2,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is significantly worse than even Model 3, the previous worst model, and after two epochs is barely better than chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 9 -- Combined tweaks\n",
    "This model has the same layers as model 6, a slightly lower learning rate, a slightly higher dropout rate, and a slightly larger batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n"
     ]
    }
   ],
   "source": [
    "# train word embeddings and load into an Embedding layer\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Training model.')\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D()(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(.6)(x)\n",
    "preds = Dense(len(authors), activation='softmax')(x)\n",
    "rms = RMSprop(lr=0.002)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='mean_squared_logarithmic_error',\n",
    "              optimizer=rms, #'rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/2\n",
      "15664/15664 [==============================] - 1182s 75ms/step - loss: 0.0792 - acc: 0.6170 - val_loss: 0.0532 - val_acc: 0.7693\n",
      "Epoch 2/2\n",
      "15664/15664 [==============================] - 1360s 87ms/step - loss: 0.0353 - acc: 0.8538 - val_loss: 0.0575 - val_acc: 0.7548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f33b0103e10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=150,\n",
    "          epochs=2,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model looked like it was going to have a higher accuracy while it was running, but the end result was a lower accuracy than most of the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model -- More Epochs\n",
    "The best model of the previous nine models was Model (BLANK), with an accuracy of (BLANK). This final model is unchanged from that model, except it is running for 10 epochs in order to acheive a higher accuracy for the submission to the contest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n"
     ]
    }
   ],
   "source": [
    "# train word embeddings and load into an Embedding layer\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Training model.')\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D()(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(.5)(x)\n",
    "preds = Dense(len(authors), activation='softmax')(x)\n",
    "rms = RMSprop(lr=0.003)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='mean_squared_logarithmic_error',\n",
    "              optimizer=rms, #'rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/10\n",
      "15664/15664 [==============================] - 1263s 81ms/step - loss: 0.0730 - acc: 0.6632 - val_loss: 0.0478 - val_acc: 0.7939\n",
      "Epoch 2/10\n",
      "15664/15664 [==============================] - 1351s 86ms/step - loss: 0.0341 - acc: 0.8580 - val_loss: 0.0459 - val_acc: 0.8107\n",
      "Epoch 3/10\n",
      "15664/15664 [==============================] - 1579s 101ms/step - loss: 0.0206 - acc: 0.9192 - val_loss: 0.0531 - val_acc: 0.7926\n",
      "Epoch 4/10\n",
      "15664/15664 [==============================] - 1267s 81ms/step - loss: 0.0134 - acc: 0.9476 - val_loss: 0.0505 - val_acc: 0.8059\n",
      "Epoch 5/10\n",
      "15664/15664 [==============================] - 2004s 128ms/step - loss: 0.0093 - acc: 0.9652 - val_loss: 0.0552 - val_acc: 0.8066\n",
      "Epoch 6/10\n",
      "15664/15664 [==============================] - 2142s 137ms/step - loss: 0.0076 - acc: 0.9720 - val_loss: 0.0559 - val_acc: 0.8056\n",
      "Epoch 7/10\n",
      "15664/15664 [==============================] - 2182s 139ms/step - loss: 0.0062 - acc: 0.9782 - val_loss: 0.0581 - val_acc: 0.8041\n",
      "Epoch 8/10\n",
      "15664/15664 [==============================] - 1306s 83ms/step - loss: 0.0057 - acc: 0.9801 - val_loss: 0.0626 - val_acc: 0.7913\n",
      "Epoch 9/10\n",
      "15664/15664 [==============================] - 1354s 86ms/step - loss: 0.0051 - acc: 0.9828 - val_loss: 0.0606 - val_acc: 0.8013\n",
      "Epoch 10/10\n",
      "15664/15664 [==============================] - 1349s 86ms/step - loss: 0.0047 - acc: 0.9844 - val_loss: 0.0625 - val_acc: 0.7974\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f621d90b550>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=100,\n",
    "          epochs=10,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, increasing the number of epochs does not appear to have improved the actual accuracy of the model, contrary to what I expected. The \"accuracy\" measurement incresed sharply (from 66% in the first epoch to the high 90s in epochs 6 and up), but the actual accuracy when validated against test data stayed around 80% the entire time. It's possible adding more epochs simply led to overtraining the model, and did not actually help improve the score.\n",
    "\n",
    "The loss rate also decreased sharply over the epochs, but I'm not convinced that will translate to a low loss rate in the actual contest submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting final model to CSV to submit to the contest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test.csv\")\n",
    "#  1. Get the sentences, \n",
    "sents = test_df['text'].tolist()\n",
    "ids = test_df['id'].tolist()\n",
    "#  2. Tokenize each sentence, \n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(sents)\n",
    "sequences = tokenizer.texts_to_sequences(sents)\n",
    "test_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(\n",
    "    model.predict(test_data)\n",
    "                           )\n",
    "predictions = predictions.rename(columns={0: 'EAP', 1: 'HPL', 2: 'MWS'})\n",
    "predictions['id'] = test_df['id']\n",
    "predictions.to_csv(\"submission_keras.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Result\n",
    "The score from the submission was 16.03046, which is significantly worse than I expected. I assume the model was overtrained and as a result couldn't generalize to the wider dataset. As noted above, I suspected that might be the case, but the degree to which it was off surprised me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Thoughts\n",
    "I tried to use a variety of methods in the submissions, but overall the convolutional network was easiest to use, although it ended up with the worst score. I wish I'd focused more of my time on trying another neural network model, using a different approach (such as a recurrent neural network). Despite my difficulties in getting the other, simpler-seeming methods to work, they ended up with much better scores than the neural network. I'd hoped to have time to get a fourth submission in, but unfortunately with the amount of time it takes to run even the simpler models on my machine, that just didn't happen.\n",
    "\n",
    "I also should have been making more submissions to the contest as I tweaked this model. Specifically, it would have been interesting to see what the score was after each epoch with the last model, and whether it improved before getting worse, or if fewer epochs would have been better all around."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
