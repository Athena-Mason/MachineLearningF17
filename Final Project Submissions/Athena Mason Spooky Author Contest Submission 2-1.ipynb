{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1\n",
    "The following code comes from the intermediate tutorial at https://www.kaggle.com/sohier/intermediate-tutorial-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from spacy import attrs\n",
    "from spacy.symbols import (VERB, NOUN, ADV, ADJ, ADP, AUX, CONJ, DET, INTJ, NUM, PART,\n",
    "PRON, PROPN, PUNCT, SCONJ, SYM, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_COLUMN = 'text'\n",
    "Y_COLUMN = 'author'\n",
    "\n",
    "def test_pipeline(df, nlp_pipeline, pipeline_name=''):\n",
    "    y = df[Y_COLUMN].copy()\n",
    "    X = pd.Series(df[TEXT_COLUMN])\n",
    "    # If you've done EDA, you may have noticed that the author classes aren't quite balanced.\n",
    "    # We'll use stratified splits just to be on the safe side.\n",
    "    rskf = StratifiedKFold(n_splits=5, random_state=1)\n",
    "    losses = []\n",
    "    for train_index, test_index in rskf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        nlp_pipeline.fit(X_train, y_train)\n",
    "        losses.append(metrics.log_loss(y_test, nlp_pipeline.predict_proba(X_test)))\n",
    "    print('{} kfolds log losses: {}'.format(pipeline_name, str([str(round(x, 3)) for x in sorted(losses)])))\n",
    "    print('{} mean log loss: {}'.format(pipeline_name, round(pd.np.mean(losses), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the stratified split is to ensure that the percentage of train and testing data is the same for each class.\n",
    "\n",
    "The purpose of a pipeline is to apply a list of transformations and provide a final estimator. All intermediate steps must apply transformation and fit methods, while the final step only needs to apply a fit. Pipelines are useful because they allow for testing the model with different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\", usecols=[TEXT_COLUMN, Y_COLUMN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams only kfolds log losses: ['0.455', '0.46', '0.47', '0.473', '0.474']\n",
      "Unigrams only mean log loss: 0.466\n"
     ]
    }
   ],
   "source": [
    "unigram_pipe = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('mnb', MultinomialNB())\n",
    "                        ])\n",
    "test_pipeline(train_df, unigram_pipe, \"Unigrams only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses mean log loss rather than accuracy as a measure. This is because the Kaggle competition uses this measurement. Unlike with accuracy, a lower loss is better. The mean log loss for this first model is .466\n",
    "\n",
    "# Model 2\n",
    "This model also comes directly from the intermediate tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramPredictions(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.unigram_mnb = Pipeline([('text', CountVectorizer()), ('mnb', MultinomialNB())])\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        # Every custom transformer requires a fit method. In this case, we want to train\n",
    "        # the naive bayes model.\n",
    "        self.unigram_mnb.fit(x, y)\n",
    "        return self\n",
    "    \n",
    "    def add_unigram_predictions(self, text_series):\n",
    "        # Resetting the index ensures the indexes equal the row numbers.\n",
    "        # This guarantees nothing will be misaligned when we merge the dataframes further down.\n",
    "        df = pd.DataFrame(text_series.reset_index(drop=True))\n",
    "        # Make unigram predicted probabilities and label them with the prediction class, aka \n",
    "        # the author.\n",
    "        unigram_predictions = pd.DataFrame(\n",
    "            self.unigram_mnb.predict_proba(text_series),\n",
    "            columns=['naive_bayes_pred_' + x for x in self.unigram_mnb.classes_]\n",
    "                                           )\n",
    "        # We only need 2 out of 3 columns, as the last is always one minus the \n",
    "        # sum of the other two. In some cases, that colinearity can actually be problematic.\n",
    "        del unigram_predictions[unigram_predictions.columns[0]]\n",
    "        df = df.merge(unigram_predictions, left_index=True, right_index=True)\n",
    "        return df\n",
    "\n",
    "    def transform(self, text_series):\n",
    "        # Every custom transformer also requires a transform method. This time we just want to \n",
    "        # provide the unigram predictions.\n",
    "        return self.add_unigram_predictions(text_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second model uses a class called UnigramPredictions which is derived from the TransformerMixin class. TransformerMixin is, as the name suggests, a mixin class for transformers (mixin classes are a form of multiple inheritence which allow classes to use methods from the mixin without the mixin being the parent class). In other words, this is a custom transformer which will be used in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the English module from spaCy\n",
    "NLP = spacy.load('en', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartOfSpeechFeatures(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.NLP = NLP\n",
    "        # Store the number of cpus available for when we do multithreading later on\n",
    "        self.num_cores = cpu_count()\n",
    "\n",
    "    def part_of_speechiness(self, pos_counts, part_of_speech):\n",
    "        if eval(part_of_speech) in pos_counts:\n",
    "            return pos_counts[eval(part_of_speech).numerator]\n",
    "        return 0\n",
    "\n",
    "    def add_pos_features(self, df):\n",
    "        text_series = df[TEXT_COLUMN]\n",
    "        \"\"\"\n",
    "        Parse each sentence with part of speech tags. \n",
    "        Using spaCy's pipe method gives us multi-threading 'for free'. \n",
    "        This is important as this is by far the single slowest step in the pipeline.\n",
    "        If you want to test this for yourself, you can use:\n",
    "            from time import time \n",
    "            start_time = time()\n",
    "            (some code)\n",
    "            print(f'Code took {time() - start_time} seconds')\n",
    "        For faster functions the timeit module would be standard... but that's\n",
    "        meant for situations where you can wait for the function to be called 1,000 times.\n",
    "        \"\"\"\n",
    "        df['doc'] = [i for i in self.NLP.pipe(text_series.values, n_threads=self.num_cores)]\n",
    "        df['pos_counts'] = df['doc'].apply(lambda x: x.count_by(attrs.POS))\n",
    "        # We get a very minor speed boost here by using pandas built in string methods\n",
    "        # instead of df['doc'].apply(len). String processing is generally slow in python,\n",
    "        # use the pandas string methods directly where possible.\n",
    "        df['sentence_length'] = df['doc'].str.len()\n",
    "        # This next step generates the fraction of each sentence that is composed of a \n",
    "        # specific part of speech.\n",
    "        # There's admittedly some voodoo in this step. Math can be more highly optimized in python\n",
    "        # than string processing, so spaCy really stores the parts of speech as numbers. If you\n",
    "        # try >>> VERB in the console you'll get 98 as the result.\n",
    "        # The monkey business with eval() here allows us to generate several named columns\n",
    "        # without specifying in advance that {'VERB': 98}.\n",
    "        for part_of_speech in ['NOUN', 'VERB', 'ADJ', 'ADV']:\n",
    "            df['{}iness'.format(part_of_speech.lower())] = df['pos_counts'].apply(\n",
    "                lambda x: self.part_of_speechiness(x, part_of_speech))\n",
    "            df['{}iness'.format(part_of_speech.lower())] /= df['sentence_length']\n",
    "        df['avg_word_length'] = (df['doc'].apply(\n",
    "            lambda x: sum([len(word) for word in x])) / df['sentence_length'])\n",
    "        return df\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        # since this transformer doesn't train a model, we don't actually need to do anything here.\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        return self.add_pos_features(df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropStringColumns(TransformerMixin):\n",
    "    # You may have noticed something odd about this class: there's no __init__!\n",
    "    # It's actually inherited from TransformerMixin, so it doesn't need to be declared again.\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        for col, dtype in zip(df.columns, df.dtypes):\n",
    "            if dtype == object:\n",
    "                del df[col]\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " kfolds log losses: ['0.458', '0.459', '0.461', '0.467', '0.468']\n",
      " mean log loss: 0.463\n"
     ]
    }
   ],
   "source": [
    "logit_all_features_pipe = Pipeline([\n",
    "        ('uni', UnigramPredictions()),\n",
    "        ('nlp', PartOfSpeechFeatures()),\n",
    "        ('clean', DropStringColumns()), \n",
    "        ('clf', LogisticRegression())\n",
    "                                     ])\n",
    "test_pipeline(train_df, logit_all_features_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is slightly better than the previous one, but only very slightly (0.003 difference in loss rate), despite the additional steps. The following model is identical to the previous model, except it uses all parts of speech instead of just nouns, adjectives, verbs, and adverbs.\n",
    "\n",
    "# Model 3\n",
    "This is the point where I start making my own changes to the model. My first change is including all the parts of speech in the PartOfSpeechFeatures transformer, rather than just noun, verbs, adjectives, and adverbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartOfSpeechFeaturesAll(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.NLP = NLP\n",
    "        # Store the number of cpus available for when we do multithreading later on\n",
    "        self.num_cores = cpu_count()\n",
    "\n",
    "    def part_of_speechiness(self, pos_counts, part_of_speech):\n",
    "        if eval(part_of_speech) in pos_counts:\n",
    "            return pos_counts[eval(part_of_speech).numerator]\n",
    "        return 0\n",
    "\n",
    "    def add_pos_features(self, df):\n",
    "        text_series = df[TEXT_COLUMN]\n",
    "\n",
    "        # Parse each sentence with part of speech tags. \n",
    "\n",
    "        df['doc'] = [i for i in self.NLP.pipe(text_series.values, n_threads=self.num_cores)]\n",
    "        df['pos_counts'] = df['doc'].apply(lambda x: x.count_by(attrs.POS))\n",
    "        # We get a very minor speed boost here by using pandas built in string methods\n",
    "        # instead of df['doc'].apply(len). String processing is generally slow in python,\n",
    "        # use the pandas string methods directly where possible.\n",
    "        df['sentence_length'] = df['doc'].str.len()\n",
    "        # This next step generates the fraction of each sentence that is composed of a \n",
    "        # specific part of speech.\n",
    "        for part_of_speech in ['NOUN', 'VERB', 'ADJ', 'ADV', 'ADP', 'AUX', 'CONJ', 'DET', 'INTJ', 'NUM', 'PART',\n",
    "                              'PRON', 'PROPN', 'PUNCT', 'SCONJ','SYM', 'X']:\n",
    "            df['{}iness'.format(part_of_speech.lower())] = df['pos_counts'].apply(\n",
    "                lambda x: self.part_of_speechiness(x, part_of_speech))\n",
    "            df['{}iness'.format(part_of_speech.lower())] /= df['sentence_length']\n",
    "        df['avg_word_length'] = (df['doc'].apply(\n",
    "            lambda x: sum([len(word) for word in x])) / df['sentence_length'])\n",
    "        return df\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        # since this transformer doesn't train a model, we don't actually need to do anything here.\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        return self.add_pos_features(df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " kfolds log losses: ['0.454', '0.455', '0.455', '0.46', '0.466']\n",
      " mean log loss: 0.458\n"
     ]
    }
   ],
   "source": [
    "logit_all_pos_features_pipe = Pipeline([\n",
    "        ('uni', UnigramPredictions()),\n",
    "        ('nlp', PartOfSpeechFeaturesAll()),\n",
    "        ('clean', DropStringColumns()), \n",
    "        ('clf', LogisticRegression())\n",
    "                                     ])\n",
    "test_pipeline(train_df, logit_all_pos_features_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model was slightly improved over the previous one (.458 vs .463). While it is not a drastic improvement, the improvement of .5 is much greater than the improvement between Models 1 and 2 (.003)\n",
    "\n",
    "# Model 4\n",
    "This continues to build on the previous model, now including bigrams in the predictions as well as unigrams. The CountVectorizer takes an \"ngram_range\" parameter which will read in ngrams of lengths between the minimum and maximum (inclusive). This CountVectorizer has been given the ngram_range (1, 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramPredictions(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.bigram_mnb = Pipeline([('text', CountVectorizer(ngram_range=(1, 2))), ('mnb', MultinomialNB())])\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        # Every custom transformer requires a fit method. In this case, we want to train\n",
    "        # the naive bayes model.\n",
    "        self.bigram_mnb.fit(x, y)\n",
    "        return self\n",
    "    \n",
    "    def add_bigram_predictions(self, text_series):\n",
    "        # Resetting the index ensures the indexes equal the row numbers.\n",
    "        # This guarantees nothing will be misaligned when we merge the dataframes further down.\n",
    "        df = pd.DataFrame(text_series.reset_index(drop=True))\n",
    "        # Make bigram predicted probabilities and label them with the prediction class, aka \n",
    "        # the author.\n",
    "        bigram_predictions = pd.DataFrame(\n",
    "            self.bigram_mnb.predict_proba(text_series),\n",
    "            columns=['naive_bayes_pred_' + x for x in self.bigram_mnb.classes_]\n",
    "                                           )\n",
    "        # We only need 2 out of 3 columns, as the last is always one minus the \n",
    "        # sum of the other two. In some cases, that colinearity can actually be problematic.\n",
    "        del bigram_predictions[bigram_predictions.columns[0]]\n",
    "        df = df.merge(bigram_predictions, left_index=True, right_index=True)\n",
    "        return df\n",
    "\n",
    "    def transform(self, text_series):\n",
    "        # Every custom transformer also requires a transform method. This time we just want to \n",
    "        # provide the bigram predictions.\n",
    "        return self.add_bigram_predictions(text_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " kfolds log losses: ['0.607', '0.611', '0.611', '0.624', '0.643']\n",
      " mean log loss: 0.619\n"
     ]
    }
   ],
   "source": [
    "logit_bigram_features_pipe = Pipeline([\n",
    "        ('bi', BigramPredictions()),\n",
    "        ('nlp', PartOfSpeechFeaturesAll()),\n",
    "        ('clean', DropStringColumns()), \n",
    "        ('clf', LogisticRegression())\n",
    "                                     ])\n",
    "test_pipeline(train_df, logit_bigram_features_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including bigrams as well as unigrams made the model significantly worse, which is not what I expected.\n",
    "\n",
    "# Model 5\n",
    "This puts the range at (2, 2), so this includes only bigrams, rather than bigrams and unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramOnlyPredictions(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.bigram_mnb = Pipeline([('text', CountVectorizer(ngram_range=(2, 2))), ('mnb', MultinomialNB())])\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        # Every custom transformer requires a fit method. In this case, we want to train\n",
    "        # the naive bayes model.\n",
    "        self.bigram_mnb.fit(x, y)\n",
    "        return self\n",
    "    \n",
    "    def add_bigram_predictions(self, text_series):\n",
    "        # Resetting the index ensures the indexes equal the row numbers.\n",
    "        # This guarantees nothing will be misaligned when we merge the dataframes further down.\n",
    "        df = pd.DataFrame(text_series.reset_index(drop=True))\n",
    "        # Make bigram predicted probabilities and label them with the prediction class, aka \n",
    "        # the author.\n",
    "        bigram_predictions = pd.DataFrame(\n",
    "            self.bigram_mnb.predict_proba(text_series),\n",
    "            columns=['naive_bayes_pred_' + x for x in self.bigram_mnb.classes_]\n",
    "                                           )\n",
    "        # We only need 2 out of 3 columns, as the last is always one minus the \n",
    "        # sum of the other two. In some cases, that colinearity can actually be problematic.\n",
    "        del bigram_predictions[bigram_predictions.columns[0]]\n",
    "        df = df.merge(bigram_predictions, left_index=True, right_index=True)\n",
    "        return df\n",
    "\n",
    "    def transform(self, text_series):\n",
    "        # Every custom transformer also requires a transform method. This time we just want to \n",
    "        # provide the bigram predictions.\n",
    "        return self.add_bigram_predictions(text_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " kfolds log losses: ['0.802', '0.808', '0.821', '0.834', '0.844']\n",
      " mean log loss: 0.822\n"
     ]
    }
   ],
   "source": [
    "logit_bigram_only_pipe = Pipeline([\n",
    "        ('bi', BigramOnlyPredictions()),\n",
    "        ('nlp', PartOfSpeechFeaturesAll()),\n",
    "        ('clean', DropStringColumns()), \n",
    "        ('clf', LogisticRegression())\n",
    "                                     ])\n",
    "test_pipeline(train_df, logit_bigram_only_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, including only bigrams and not unigrams made the model even worse.\n",
    "\n",
    "# Model 6\n",
    "This model includes unigrams, bigrams, and trigrams. I suspect it won't be much better than the model with bigrams and  unigrams, and may in fact be worse, but it's worth looking at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramPredictions(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.trigram_mnb = Pipeline([('text', CountVectorizer(ngram_range=(1, 3))), ('mnb', MultinomialNB())])\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        # Every custom transformer requires a fit method. In this case, we want to train\n",
    "        # the naive bayes model.\n",
    "        self.trigram_mnb.fit(x, y)\n",
    "        return self\n",
    "    \n",
    "    def add_trigram_predictions(self, text_series):\n",
    "        # Resetting the index ensures the indexes equal the row numbers.\n",
    "        # This guarantees nothing will be misaligned when we merge the dataframes further down.\n",
    "        df = pd.DataFrame(text_series.reset_index(drop=True))\n",
    "        # Make bigram predicted probabilities and label them with the prediction class, aka \n",
    "        # the author.\n",
    "        trigram_predictions = pd.DataFrame(\n",
    "            self.trigram_mnb.predict_proba(text_series),\n",
    "            columns=['naive_bayes_pred_' + x for x in self.trigram_mnb.classes_]\n",
    "                                           )\n",
    "        # We only need 2 out of 3 columns, as the last is always one minus the \n",
    "        # sum of the other two. In some cases, that colinearity can actually be problematic.\n",
    "        del trigram_predictions[trigram_predictions.columns[0]]\n",
    "        df = df.merge(trigram_predictions, left_index=True, right_index=True)\n",
    "        return df\n",
    "\n",
    "    def transform(self, text_series):\n",
    "        # Every custom transformer also requires a transform method. This time we just want to \n",
    "        # provide the bigram predictions.\n",
    "        return self.add_trigram_predictions(text_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " kfolds log losses: ['0.776', '0.785', '0.79', '0.806', '0.819']\n",
      " mean log loss: 0.795\n"
     ]
    }
   ],
   "source": [
    "logit_trigram_pipe = Pipeline([\n",
    "        ('tri', TrigramPredictions()),\n",
    "        ('nlp', PartOfSpeechFeaturesAll()),\n",
    "        ('clean', DropStringColumns()), \n",
    "        ('clf', LogisticRegression())\n",
    "                                     ])\n",
    "test_pipeline(train_df, logit_trigram_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since playing with the size of the ngrams only seemed to make it worse, I'm going to test the best model so far (Model 3) with different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramPredictionsNeighbors(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.unigram_bt = Pipeline([('text', CountVectorizer()), ('bt', KNeighborsClassifier())])\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        # Every custom transformer requires a fit method. In this case, we want to train\n",
    "        # the naive bayes model.\n",
    "        self.unigram_bt.fit(x, y)\n",
    "        return self\n",
    "    \n",
    "    def add_unigram_predictions(self, text_series):\n",
    "        # Resetting the index ensures the indexes equal the row numbers.\n",
    "        # This guarantees nothing will be misaligned when we merge the dataframes further down.\n",
    "        df = pd.DataFrame(text_series.reset_index(drop=True))\n",
    "        # Make unigram predicted probabilities and label them with the prediction class, aka \n",
    "        # the author.\n",
    "        unigram_predictions = pd.DataFrame(\n",
    "            self.unigram_bt.predict_proba(text_series),\n",
    "            columns=['neighbor_pred_' + x for x in self.unigram_bt.classes_]\n",
    "                                           )\n",
    "        # We only need 2 out of 3 columns, as the last is always one minus the \n",
    "        # sum of the other two. In some cases, that colinearity can actually be problematic.\n",
    "        del unigram_predictions[unigram_predictions.columns[0]]\n",
    "        df = df.merge(unigram_predictions, left_index=True, right_index=True)\n",
    "        return df\n",
    "\n",
    "    def transform(self, text_series):\n",
    "        # Every custom transformer also requires a transform method. This time we just want to \n",
    "        # provide the unigram predictions.\n",
    "        return self.add_unigram_predictions(text_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_neighbors_pipe = Pipeline([\n",
    "        ('upn', UnigramPredictionsNeighbors()),\n",
    "        ('nlp', PartOfSpeechFeaturesAll()),\n",
    "        ('clean', DropStringColumns()), \n",
    "        ('clf', LogisticRegression())\n",
    "                                     ])\n",
    "test_pipeline(train_df, logit_neighbors_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason I am not sure of, the kernel kept dying when I tried to run the above pipeline. No errors were thrown, the kernal just stopped working. This happened four times before I gave up. I haven't had this issue with the other pipelines, and I'm not sure what caused it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission_df(trained_prediction_pipeline, test_df):\n",
    "    predictions = pd.DataFrame(\n",
    "        trained_prediction_pipeline.predict_proba(test_df.text),\n",
    "        columns=trained_prediction_pipeline.classes_\n",
    "                               )\n",
    "    predictions['id'] = test_df['id']\n",
    "    predictions.to_csv(\"submission_pipeline.csv\", index=False)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.040407</td>\n",
       "      <td>0.030331</td>\n",
       "      <td>0.929263</td>\n",
       "      <td>id02310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.962420</td>\n",
       "      <td>0.016341</td>\n",
       "      <td>0.021238</td>\n",
       "      <td>id24541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.016281</td>\n",
       "      <td>0.970191</td>\n",
       "      <td>0.013528</td>\n",
       "      <td>id00134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.056016</td>\n",
       "      <td>0.929973</td>\n",
       "      <td>0.014011</td>\n",
       "      <td>id27757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.906087</td>\n",
       "      <td>0.081219</td>\n",
       "      <td>0.012694</td>\n",
       "      <td>id04081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.963324</td>\n",
       "      <td>0.023751</td>\n",
       "      <td>0.012924</td>\n",
       "      <td>id27337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.948236</td>\n",
       "      <td>0.037787</td>\n",
       "      <td>0.013977</td>\n",
       "      <td>id24265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.029865</td>\n",
       "      <td>0.032327</td>\n",
       "      <td>0.937809</td>\n",
       "      <td>id25917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.967221</td>\n",
       "      <td>0.011587</td>\n",
       "      <td>0.021192</td>\n",
       "      <td>id04951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.953672</td>\n",
       "      <td>0.013473</td>\n",
       "      <td>0.032856</td>\n",
       "      <td>id14549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.030659</td>\n",
       "      <td>0.014330</td>\n",
       "      <td>0.955011</td>\n",
       "      <td>id22505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.013288</td>\n",
       "      <td>0.973283</td>\n",
       "      <td>0.013429</td>\n",
       "      <td>id24002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.039396</td>\n",
       "      <td>0.049248</td>\n",
       "      <td>0.911356</td>\n",
       "      <td>id18982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.017445</td>\n",
       "      <td>0.972473</td>\n",
       "      <td>0.010082</td>\n",
       "      <td>id15181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.776899</td>\n",
       "      <td>0.069726</td>\n",
       "      <td>0.153376</td>\n",
       "      <td>id21888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.047769</td>\n",
       "      <td>0.019918</td>\n",
       "      <td>0.932313</td>\n",
       "      <td>id12035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.187822</td>\n",
       "      <td>0.013224</td>\n",
       "      <td>0.798953</td>\n",
       "      <td>id17991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.968050</td>\n",
       "      <td>0.018055</td>\n",
       "      <td>0.013894</td>\n",
       "      <td>id10707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.258905</td>\n",
       "      <td>0.020437</td>\n",
       "      <td>0.720658</td>\n",
       "      <td>id07101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.464430</td>\n",
       "      <td>0.519670</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>id00345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.951512</td>\n",
       "      <td>0.020686</td>\n",
       "      <td>0.027801</td>\n",
       "      <td>id05912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.962483</td>\n",
       "      <td>0.023427</td>\n",
       "      <td>0.014090</td>\n",
       "      <td>id13443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.142353</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>0.845447</td>\n",
       "      <td>id09248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.038757</td>\n",
       "      <td>0.948001</td>\n",
       "      <td>0.013242</td>\n",
       "      <td>id17542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.049095</td>\n",
       "      <td>0.012051</td>\n",
       "      <td>0.938854</td>\n",
       "      <td>id06995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.785469</td>\n",
       "      <td>0.058890</td>\n",
       "      <td>0.155642</td>\n",
       "      <td>id25159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.953927</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>0.017812</td>\n",
       "      <td>id25729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.952858</td>\n",
       "      <td>0.012408</td>\n",
       "      <td>0.034734</td>\n",
       "      <td>id26949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.027736</td>\n",
       "      <td>0.021522</td>\n",
       "      <td>0.950742</td>\n",
       "      <td>id27191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.030063</td>\n",
       "      <td>0.014432</td>\n",
       "      <td>0.955505</td>\n",
       "      <td>id07668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8362</th>\n",
       "      <td>0.036038</td>\n",
       "      <td>0.036241</td>\n",
       "      <td>0.927721</td>\n",
       "      <td>id22510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8363</th>\n",
       "      <td>0.970222</td>\n",
       "      <td>0.016440</td>\n",
       "      <td>0.013338</td>\n",
       "      <td>id19204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8364</th>\n",
       "      <td>0.034968</td>\n",
       "      <td>0.028727</td>\n",
       "      <td>0.936305</td>\n",
       "      <td>id05758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8365</th>\n",
       "      <td>0.864099</td>\n",
       "      <td>0.048956</td>\n",
       "      <td>0.086945</td>\n",
       "      <td>id27063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8366</th>\n",
       "      <td>0.030056</td>\n",
       "      <td>0.014783</td>\n",
       "      <td>0.955161</td>\n",
       "      <td>id11773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8367</th>\n",
       "      <td>0.122395</td>\n",
       "      <td>0.015281</td>\n",
       "      <td>0.862324</td>\n",
       "      <td>id11562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8368</th>\n",
       "      <td>0.054801</td>\n",
       "      <td>0.008882</td>\n",
       "      <td>0.936317</td>\n",
       "      <td>id16208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8369</th>\n",
       "      <td>0.029929</td>\n",
       "      <td>0.023523</td>\n",
       "      <td>0.946548</td>\n",
       "      <td>id04036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8370</th>\n",
       "      <td>0.921510</td>\n",
       "      <td>0.023907</td>\n",
       "      <td>0.054584</td>\n",
       "      <td>id26159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8371</th>\n",
       "      <td>0.468381</td>\n",
       "      <td>0.234820</td>\n",
       "      <td>0.296799</td>\n",
       "      <td>id26777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8372</th>\n",
       "      <td>0.928182</td>\n",
       "      <td>0.022232</td>\n",
       "      <td>0.049586</td>\n",
       "      <td>id08501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8373</th>\n",
       "      <td>0.029667</td>\n",
       "      <td>0.957492</td>\n",
       "      <td>0.012841</td>\n",
       "      <td>id11216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8374</th>\n",
       "      <td>0.945554</td>\n",
       "      <td>0.020294</td>\n",
       "      <td>0.034152</td>\n",
       "      <td>id03410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8375</th>\n",
       "      <td>0.044845</td>\n",
       "      <td>0.016748</td>\n",
       "      <td>0.938407</td>\n",
       "      <td>id04537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8376</th>\n",
       "      <td>0.962430</td>\n",
       "      <td>0.019429</td>\n",
       "      <td>0.018140</td>\n",
       "      <td>id26628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8377</th>\n",
       "      <td>0.026616</td>\n",
       "      <td>0.961507</td>\n",
       "      <td>0.011877</td>\n",
       "      <td>id01586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8378</th>\n",
       "      <td>0.055188</td>\n",
       "      <td>0.032181</td>\n",
       "      <td>0.912631</td>\n",
       "      <td>id13421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8379</th>\n",
       "      <td>0.021076</td>\n",
       "      <td>0.962989</td>\n",
       "      <td>0.015935</td>\n",
       "      <td>id26084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8380</th>\n",
       "      <td>0.035526</td>\n",
       "      <td>0.020683</td>\n",
       "      <td>0.943791</td>\n",
       "      <td>id05375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8381</th>\n",
       "      <td>0.958642</td>\n",
       "      <td>0.017873</td>\n",
       "      <td>0.023485</td>\n",
       "      <td>id23212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8382</th>\n",
       "      <td>0.942369</td>\n",
       "      <td>0.040033</td>\n",
       "      <td>0.017598</td>\n",
       "      <td>id15980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8383</th>\n",
       "      <td>0.014752</td>\n",
       "      <td>0.969190</td>\n",
       "      <td>0.016059</td>\n",
       "      <td>id11719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8384</th>\n",
       "      <td>0.948825</td>\n",
       "      <td>0.036430</td>\n",
       "      <td>0.014745</td>\n",
       "      <td>id13109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8385</th>\n",
       "      <td>0.968744</td>\n",
       "      <td>0.014260</td>\n",
       "      <td>0.016996</td>\n",
       "      <td>id07156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8386</th>\n",
       "      <td>0.122807</td>\n",
       "      <td>0.867730</td>\n",
       "      <td>0.009463</td>\n",
       "      <td>id04893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8387</th>\n",
       "      <td>0.879443</td>\n",
       "      <td>0.021613</td>\n",
       "      <td>0.098944</td>\n",
       "      <td>id11749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8388</th>\n",
       "      <td>0.063311</td>\n",
       "      <td>0.016834</td>\n",
       "      <td>0.919854</td>\n",
       "      <td>id10526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8389</th>\n",
       "      <td>0.952201</td>\n",
       "      <td>0.033705</td>\n",
       "      <td>0.014094</td>\n",
       "      <td>id13477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8390</th>\n",
       "      <td>0.025892</td>\n",
       "      <td>0.026297</td>\n",
       "      <td>0.947811</td>\n",
       "      <td>id13761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8391</th>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.973811</td>\n",
       "      <td>0.010089</td>\n",
       "      <td>id04282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8392 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           EAP       HPL       MWS       id\n",
       "0     0.040407  0.030331  0.929263  id02310\n",
       "1     0.962420  0.016341  0.021238  id24541\n",
       "2     0.016281  0.970191  0.013528  id00134\n",
       "3     0.056016  0.929973  0.014011  id27757\n",
       "4     0.906087  0.081219  0.012694  id04081\n",
       "5     0.963324  0.023751  0.012924  id27337\n",
       "6     0.948236  0.037787  0.013977  id24265\n",
       "7     0.029865  0.032327  0.937809  id25917\n",
       "8     0.967221  0.011587  0.021192  id04951\n",
       "9     0.953672  0.013473  0.032856  id14549\n",
       "10    0.030659  0.014330  0.955011  id22505\n",
       "11    0.013288  0.973283  0.013429  id24002\n",
       "12    0.039396  0.049248  0.911356  id18982\n",
       "13    0.017445  0.972473  0.010082  id15181\n",
       "14    0.776899  0.069726  0.153376  id21888\n",
       "15    0.047769  0.019918  0.932313  id12035\n",
       "16    0.187822  0.013224  0.798953  id17991\n",
       "17    0.968050  0.018055  0.013894  id10707\n",
       "18    0.258905  0.020437  0.720658  id07101\n",
       "19    0.464430  0.519670  0.015900  id00345\n",
       "20    0.951512  0.020686  0.027801  id05912\n",
       "21    0.962483  0.023427  0.014090  id13443\n",
       "22    0.142353  0.012200  0.845447  id09248\n",
       "23    0.038757  0.948001  0.013242  id17542\n",
       "24    0.049095  0.012051  0.938854  id06995\n",
       "25    0.785469  0.058890  0.155642  id25159\n",
       "26    0.953927  0.028261  0.017812  id25729\n",
       "27    0.952858  0.012408  0.034734  id26949\n",
       "28    0.027736  0.021522  0.950742  id27191\n",
       "29    0.030063  0.014432  0.955505  id07668\n",
       "...        ...       ...       ...      ...\n",
       "8362  0.036038  0.036241  0.927721  id22510\n",
       "8363  0.970222  0.016440  0.013338  id19204\n",
       "8364  0.034968  0.028727  0.936305  id05758\n",
       "8365  0.864099  0.048956  0.086945  id27063\n",
       "8366  0.030056  0.014783  0.955161  id11773\n",
       "8367  0.122395  0.015281  0.862324  id11562\n",
       "8368  0.054801  0.008882  0.936317  id16208\n",
       "8369  0.029929  0.023523  0.946548  id04036\n",
       "8370  0.921510  0.023907  0.054584  id26159\n",
       "8371  0.468381  0.234820  0.296799  id26777\n",
       "8372  0.928182  0.022232  0.049586  id08501\n",
       "8373  0.029667  0.957492  0.012841  id11216\n",
       "8374  0.945554  0.020294  0.034152  id03410\n",
       "8375  0.044845  0.016748  0.938407  id04537\n",
       "8376  0.962430  0.019429  0.018140  id26628\n",
       "8377  0.026616  0.961507  0.011877  id01586\n",
       "8378  0.055188  0.032181  0.912631  id13421\n",
       "8379  0.021076  0.962989  0.015935  id26084\n",
       "8380  0.035526  0.020683  0.943791  id05375\n",
       "8381  0.958642  0.017873  0.023485  id23212\n",
       "8382  0.942369  0.040033  0.017598  id15980\n",
       "8383  0.014752  0.969190  0.016059  id11719\n",
       "8384  0.948825  0.036430  0.014745  id13109\n",
       "8385  0.968744  0.014260  0.016996  id07156\n",
       "8386  0.122807  0.867730  0.009463  id04893\n",
       "8387  0.879443  0.021613  0.098944  id11749\n",
       "8388  0.063311  0.016834  0.919854  id10526\n",
       "8389  0.952201  0.033705  0.014094  id13477\n",
       "8390  0.025892  0.026297  0.947811  id13761\n",
       "8391  0.016100  0.973811  0.010089  id04282\n",
       "\n",
       "[8392 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_submission_df(logit_all_pos_features_pipe, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the issues with the kernal stopping when I tried to run another classifier, I ended up submitting Model 3 to the competition. It received a score of 0.47390, which is slightly worse than the score of .458 identified in this notebook. At the time of the submission, it was number 600 on the leaderboard.\n",
    "\n",
    "Overall, the pipeline seemed like a relatively simple method, but I had trouble wrapping my head around how to make the transformations work with one another, and I'm still not sure why the kernel kept dying when I tried to use the KNeighborsClassifier. I also tried to play around with some of the other classifiers (GaussianNB and BallTree), but couldn't get either of them to run without throwing errors. Overall, this model was the worst in terms of effort vs accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
