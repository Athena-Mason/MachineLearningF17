{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Frequency (unmodified)\n",
    "This first attempt is based on the beginner's tutorial linked in the Kaggle contest overview: https://www.kaggle.com/rtatman/beginner-s-tutorial-python\n",
    "\n",
    "The beginner's tutorial uses the simple method of counting up how often each word is used by each author and using this to generate probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in some helpful libraries\n",
    "import nltk # the natural langauage toolkit, open-source NLP\n",
    "import pandas as pd # dataframes\n",
    "\n",
    "# read our data into a dataframe\n",
    "texts = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# look at the first few rows\n",
    "texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split data\n",
    "\n",
    "# split the data by author\n",
    "byAuthor = texts.groupby(\"author\")\n",
    "\n",
    "### Tokenize (split into individual words) our text\n",
    "\n",
    "# word frequency by author\n",
    "wordFreqByAuthor = nltk.probability.ConditionalFreqDist()\n",
    "\n",
    "# for each author...\n",
    "for name, group in byAuthor:\n",
    "    # get all of the sentences they wrote and collapse them into a\n",
    "    # single long string\n",
    "    sentences = group['text'].str.cat(sep = ' ')\n",
    "    \n",
    "    # convert everything to lower case (so \"The\" and \"the\" get counted as \n",
    "    # the same word rather than two different words)\n",
    "    sentences = sentences.lower()\n",
    "    \n",
    "    # split the text into individual tokens    \n",
    "    tokens = nltk.tokenize.word_tokenize(sentences)\n",
    "    \n",
    "    # calculate the frequency of each token\n",
    "    frequency = nltk.FreqDist(tokens)\n",
    "\n",
    "    # add the frequencies for each author to our dictionary\n",
    "    wordFreqByAuthor[name] = (frequency)\n",
    "    \n",
    "# now we have an dictionary where each entry is the frequency distrobution\n",
    "# of words for a specific author.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blood: EAP\n",
      "0.00014646397201676582\n",
      "blood: MWS\n",
      "0.00022773011333545174\n",
      "blood: HPL\n",
      "0.00022992337803427008\n",
      "\n",
      "scream: EAP\n",
      "1.7231055531384214e-05\n",
      "scream: MWS\n",
      "2.6480245736680435e-05\n",
      "scream: HPL\n",
      "9.196935121370803e-05\n",
      "\n",
      "fear: EAP\n",
      "0.00010338633318830528\n",
      "fear: MWS\n",
      "0.0006196377502383222\n",
      "fear: HPL\n",
      "0.0005748084450856752\n"
     ]
    }
   ],
   "source": [
    "# see how often each author says \"blood\"\n",
    "for i in wordFreqByAuthor.keys():\n",
    "    print(\"blood: \" + i)\n",
    "    print(wordFreqByAuthor[i].freq('blood'))\n",
    "\n",
    "# print a blank line\n",
    "print()\n",
    "\n",
    "# see how often each author says \"scream\"\n",
    "for i in wordFreqByAuthor.keys():\n",
    "    print(\"scream: \" + i)\n",
    "    print(wordFreqByAuthor[i].freq('scream'))\n",
    "    \n",
    "# print a blank line\n",
    "print()\n",
    "\n",
    "# see how often each author says \"fear\"\n",
    "for i in wordFreqByAuthor.keys():\n",
    "    print(\"fear: \" + i)\n",
    "    print(wordFreqByAuthor[i].freq('fear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HPL'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One way to guess authorship is to use the joint probabilty that each \n",
    "# author used each word in a given sentence.\n",
    "\n",
    "# first, let's start with a test sentence\n",
    "testSentence = \"It was a dark and stormy night.\"\n",
    "\n",
    "# and then lowercase & tokenize our test sentence\n",
    "preProcessedTestSentence = nltk.tokenize.word_tokenize(testSentence.lower())\n",
    "\n",
    "# create an empy dataframe to put our output in\n",
    "testProbailities = pd.DataFrame(columns = ['author','word','probability'])\n",
    "\n",
    "# For each author...\n",
    "for i in wordFreqByAuthor.keys():\n",
    "    # for each word in our test sentence...\n",
    "    for j  in preProcessedTestSentence:\n",
    "        # find out how frequently the author used that word\n",
    "        wordFreq = wordFreqByAuthor[i].freq(j)\n",
    "        # and add a very small amount to every prob. so none of them are 0\n",
    "        smoothedWordFreq = wordFreq + 0.000001\n",
    "        # add the author, word and smoothed freq. to our dataframe\n",
    "        output = pd.DataFrame([[i, j, smoothedWordFreq]], columns = ['author','word','probability'])\n",
    "        testProbailities = testProbailities.append(output, ignore_index = True)\n",
    "\n",
    "# empty dataframe for the probability that each author wrote the sentence\n",
    "testProbailitiesByAuthor = pd.DataFrame(columns = ['author','jointProbability'])\n",
    "\n",
    "# now let's group the dataframe with our frequency by author\n",
    "for i in wordFreqByAuthor.keys():\n",
    "    # get the joint probability that each author wrote each word\n",
    "    oneAuthor = testProbailities.query('author == \"' + i + '\"')\n",
    "    jointProbability = oneAuthor.product(numeric_only = True)[0]\n",
    "    \n",
    "    # and add that to our dataframe\n",
    "    output = pd.DataFrame([[i, jointProbability]], columns = ['author','jointProbability'])\n",
    "    testProbailitiesByAuthor = testProbailitiesByAuthor.append(output, ignore_index = True)\n",
    "\n",
    "# and our winner is...\n",
    "testProbailitiesByAuthor.loc[testProbailitiesByAuthor['jointProbability'].idxmax(),'author']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code was directly from the beginner tutorial, with some minor adjustments to make it run (there was an issue with running \"tokenize\" and it needed to be decoded into Unicode from ASCII in order to run properly). For obvious reasons, the beginner code was very basic. It only tested the probability for once sentence, and did not print the probabilities (only the winning result).\n",
    "\n",
    "Because the example only printed the name of the predicted author, I also printed the dataframe to see what it came up with for each author:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  author  jointProbability\n",
      "0    EAP      1.331812e-21\n",
      "1    MWS      1.748100e-21\n",
      "2    HPL      2.483591e-20\n"
     ]
    }
   ],
   "source": [
    "print(testProbailitiesByAuthor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I will see how this fares when split into training and testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16389</th>\n",
       "      <td>id10465</td>\n",
       "      <td>I looked round on the audience; the females we...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15290</th>\n",
       "      <td>id23479</td>\n",
       "      <td>The bent, goatish giant before him seemed like...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9080</th>\n",
       "      <td>id04706</td>\n",
       "      <td>Over and above the fumes and sickening closene...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17587</th>\n",
       "      <td>id12742</td>\n",
       "      <td>That Raymond should marry Idris was more than ...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16187</th>\n",
       "      <td>id00778</td>\n",
       "      <td>Whilst I had hitherto considered this but a na...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               text author\n",
       "16389  id10465  I looked round on the audience; the females we...    MWS\n",
       "15290  id23479  The bent, goatish giant before him seemed like...    HPL\n",
       "9080   id04706  Over and above the fumes and sickening closene...    HPL\n",
       "17587  id12742  That Raymond should marry Idris was more than ...    MWS\n",
       "16187  id00778  Whilst I had hitherto considered this but a na...    HPL"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "text_shuffle = shuffle(texts)\n",
    "\n",
    "print(text_shuffle.shape)\n",
    "text_shuffle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15663, 3) (3916, 3)\n"
     ]
    }
   ],
   "source": [
    "# We will use 80% of the labeled data for training and 20% for testing\n",
    "train_size = math.floor(text_shuffle.shape[0] * .8)\n",
    "train = text_shuffle.iloc[:train_size, :]\n",
    "test = text_shuffle.iloc[train_size:, :]\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split data\n",
    "\n",
    "# split the data by author\n",
    "byAuthor = train.groupby(\"author\")\n",
    "\n",
    "### Tokenize (split into individual words) our text\n",
    "\n",
    "# word frequency by author\n",
    "wordFreqByAuthor = nltk.probability.ConditionalFreqDist()\n",
    "\n",
    "# for each author...\n",
    "for name, group in byAuthor:\n",
    "    # get all of the sentences they wrote and collapse them into a\n",
    "    # single long string\n",
    "    sentences = group['text'].str.cat(sep = ' ')\n",
    "    \n",
    "    # convert everything to lower case (so \"The\" and \"the\" get counted as \n",
    "    # the same word rather than two different words)\n",
    "    sentences = sentences.lower()\n",
    "    \n",
    "    # split the text into individual tokens    \n",
    "    tokens = nltk.tokenize.word_tokenize(sentences)\n",
    "    \n",
    "    # calculate the frequency of each token\n",
    "    frequency = nltk.FreqDist(tokens)\n",
    "\n",
    "    # add the frequencies for each author to our dictionary\n",
    "    wordFreqByAuthor[name] = (frequency)\n",
    "    \n",
    "# now we have an dictionary where each entry is the frequency distrobution\n",
    "# of words for a specific author.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love my cousin tenderly and sincerely.\n",
      "['i', 'love', 'my', 'cousin', 'tenderly', 'and', 'sincerely', '.']\n"
     ]
    }
   ],
   "source": [
    "# first see if this works with the first sentence from test\n",
    "testSentence = test.iloc[0]['text']\n",
    "print(testSentence)\n",
    "\n",
    "print(nltk.tokenize.word_tokenize(testSentence.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  author  jointProbability\n",
      "0    EAP      6.876213e-27\n",
      "1    MWS      2.647353e-23\n",
      "2    HPL      1.486582e-27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'MWS'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and then lowercase & tokenize our test sentence\n",
    "preProcessedTestSentence = nltk.tokenize.word_tokenize(testSentence.lower())\n",
    "\n",
    "# create an empy dataframe to put our output in\n",
    "testProbailities = pd.DataFrame(columns = ['author','word','probability'])\n",
    "\n",
    "# For each author...\n",
    "for i in wordFreqByAuthor.keys():\n",
    "    # for each word in our test sentence...\n",
    "    for j  in preProcessedTestSentence:\n",
    "        # find out how frequently the author used that word\n",
    "        wordFreq = wordFreqByAuthor[i].freq(j)\n",
    "        # and add a very small amount to every prob. so none of them are 0\n",
    "        smoothedWordFreq = wordFreq + 0.000001\n",
    "        # add the author, word and smoothed freq. to our dataframe\n",
    "        output = pd.DataFrame([[i, j, smoothedWordFreq]], columns = ['author','word','probability'])\n",
    "        testProbailities = testProbailities.append(output, ignore_index = True)\n",
    "\n",
    "# empty dataframe for the probability that each author wrote the sentence\n",
    "testProbailitiesByAuthor = pd.DataFrame(columns = ['author','jointProbability'])\n",
    "\n",
    "# now let's group the dataframe with our frequency by author\n",
    "for i in wordFreqByAuthor.keys():\n",
    "    # get the joint probability that each author wrote each word\n",
    "    oneAuthor = testProbailities.query('author == \"' + i + '\"')\n",
    "    jointProbability = oneAuthor.product(numeric_only = True)[0]\n",
    "    \n",
    "    # and add that to our dataframe\n",
    "    output = pd.DataFrame([[i, jointProbability]], columns = ['author','jointProbability'])\n",
    "    testProbailitiesByAuthor = testProbailitiesByAuthor.append(output, ignore_index = True)\n",
    "\n",
    "# and our winner is...\n",
    "print(testProbailitiesByAuthor)\n",
    "testProbailitiesByAuthor.loc[testProbailitiesByAuthor['jointProbability'].idxmax(),'author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MWS\n"
     ]
    }
   ],
   "source": [
    "# was it right?\n",
    "print(test.iloc[0]['author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was! Now let's test the accuracy overall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8480592441266599"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "\n",
    "for index, row in test.iterrows():\n",
    "    testSentence = row['text']\n",
    "    # and then lowercase & tokenize our test sentence\n",
    "    preProcessedTestSentence = nltk.tokenize.word_tokenize(testSentence.lower())\n",
    "\n",
    "    # create an empy dataframe to put our output in\n",
    "    testProbailities = pd.DataFrame(columns = ['author','word','probability'])\n",
    "\n",
    "    # For each author...\n",
    "    for i in wordFreqByAuthor.keys():\n",
    "        # for each word in our test sentence...\n",
    "        for j  in preProcessedTestSentence:\n",
    "            # find out how frequently the author used that word\n",
    "            wordFreq = wordFreqByAuthor[i].freq(j)\n",
    "            # and add a very small amount to every prob. so none of them are 0\n",
    "            smoothedWordFreq = wordFreq + 0.000001\n",
    "            # add the author, word and smoothed freq. to our dataframe\n",
    "            output = pd.DataFrame([[i, j, smoothedWordFreq]], columns = ['author','word','probability'])\n",
    "            testProbailities = testProbailities.append(output, ignore_index = True)\n",
    "\n",
    "    # empty dataframe for the probability that each author wrote the sentence\n",
    "    testProbailitiesByAuthor = pd.DataFrame(columns = ['author','jointProbability'])\n",
    "\n",
    "    # now let's group the dataframe with our frequency by author\n",
    "    for i in wordFreqByAuthor.keys():\n",
    "        # get the joint probability that each author wrote each word\n",
    "        oneAuthor = testProbailities.query('author == \"' + i + '\"')\n",
    "        jointProbability = oneAuthor.product(numeric_only = True)[0]\n",
    "\n",
    "        # and add that to our dataframe\n",
    "        output = pd.DataFrame([[i, jointProbability]], columns = ['author','jointProbability'])\n",
    "        testProbailitiesByAuthor = testProbailitiesByAuthor.append(output, ignore_index = True)\n",
    "\n",
    "    # and our winner is...\n",
    "    pred = testProbailitiesByAuthor.loc[testProbailitiesByAuthor['jointProbability'].idxmax(),'author']\n",
    "    if(pred == row['author']):\n",
    "        correct = correct + 1\n",
    "        \n",
    "accuracy = correct / test.shape[0]\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the simplicity of the model, this is better than I expected. However, there are ways to improve it while sticking to the same basic concept.\n",
    "\n",
    "Some simple ways this could be improved are:\n",
    "* Removing stopwords from the data before calculating the word frequency by author\n",
    "* Incorporating features such as the length of the sentences or lexical diversity\n",
    "* Looking at long words separately, as these are more likely to be unique\n",
    "* Collocations\n",
    "* Overall lengths of words, frequency of word lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Frequency (minus stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15663, 3) (3916, 3)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "# We will use 80% of the labeled data for training and 20% for testing\n",
    "train_size = math.floor(text_shuffle.shape[0] * .8)\n",
    "train = text_shuffle.iloc[:train_size, :]\n",
    "test = text_shuffle.iloc[train_size:, :]\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split data\n",
    "\n",
    "# split the data by author\n",
    "byAuthor = train.groupby(\"author\")\n",
    "\n",
    "### Tokenize (split into individual words) our text\n",
    "\n",
    "# word frequency by author\n",
    "wordFreqByAuthor = nltk.probability.ConditionalFreqDist()\n",
    "\n",
    "# for each author...\n",
    "for name, group in byAuthor:\n",
    "    # get all of the sentences they wrote and collapse them into a\n",
    "    # single long string\n",
    "    sentences = group['text'].str.cat(sep = ' ')\n",
    "    \n",
    "    # convert everything to lower case (so \"The\" and \"the\" get counted as \n",
    "    # the same word rather than two different words)\n",
    "    sentences = sentences.lower()\n",
    "    \n",
    "    # split the text into individual tokens    \n",
    "    tokens = nltk.tokenize.word_tokenize(sentences)\n",
    "    \n",
    "    #remove stopwords from tokens\n",
    "    tokens_modified = []\n",
    "    for i in tokens:\n",
    "        if i not in stops:\n",
    "            tokens_modified.append(i)\n",
    "    \n",
    "    # calculate the frequency of each token\n",
    "    frequency = nltk.FreqDist(tokens_modified)\n",
    "\n",
    "    # add the frequencies for each author to our dictionary\n",
    "    wordFreqByAuthor[name] = (frequency)\n",
    "    \n",
    "# now we have an dictionary where each entry is the frequency distribution\n",
    "# of words for a specific author, minus stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8192032686414709"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing accuracy without stopwords\n",
    "correct = 0\n",
    "\n",
    "for index, row in test.iterrows():\n",
    "    testSentence = row['text']\n",
    "    # and then lowercase & tokenize our test sentence\n",
    "    preProcessedTestSentence = nltk.tokenize.word_tokenize(testSentence.lower())\n",
    "\n",
    "    # create an empy dataframe to put our output in\n",
    "    testProbailities = pd.DataFrame(columns = ['author','word','probability'])\n",
    "\n",
    "    # For each author...\n",
    "    for i in wordFreqByAuthor.keys():\n",
    "        # for each word in our test sentence...\n",
    "        for j  in preProcessedTestSentence:\n",
    "            # find out how frequently the author used that word\n",
    "            wordFreq = wordFreqByAuthor[i].freq(j)\n",
    "            # and add a very small amount to every prob. so none of them are 0\n",
    "            smoothedWordFreq = wordFreq + 0.000001\n",
    "            # add the author, word and smoothed freq. to our dataframe\n",
    "            output = pd.DataFrame([[i, j, smoothedWordFreq]], columns = ['author','word','probability'])\n",
    "            testProbailities = testProbailities.append(output, ignore_index = True)\n",
    "\n",
    "    # empty dataframe for the probability that each author wrote the sentence\n",
    "    testProbailitiesByAuthor = pd.DataFrame(columns = ['author','jointProbability'])\n",
    "\n",
    "    # now let's group the dataframe with our frequency by author\n",
    "    for i in wordFreqByAuthor.keys():\n",
    "        # get the joint probability that each author wrote each word\n",
    "        oneAuthor = testProbailities.query('author == \"' + i + '\"')\n",
    "        jointProbability = oneAuthor.product(numeric_only = True)[0]\n",
    "\n",
    "        # and add that to our dataframe\n",
    "        output = pd.DataFrame([[i, jointProbability]], columns = ['author','jointProbability'])\n",
    "        testProbailitiesByAuthor = testProbailitiesByAuthor.append(output, ignore_index = True)\n",
    "\n",
    "    # and our winner is...\n",
    "    pred = testProbailitiesByAuthor.loc[testProbailitiesByAuthor['jointProbability'].idxmax(),'author']\n",
    "    if(pred == row['author']):\n",
    "        correct = correct + 1\n",
    "        \n",
    "accuracy = correct / test.shape[0]\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, it appears that removing the stopwords made the model less accurate. Given that this is relying solely on word frequency, this does make sense, since different authors may use these words at different rates. That is, the stopwords themselves are not indicative of any particular author, but their frequency might be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams\n",
    "This model does not involve taking out the stop words, as that drastically decreased the effectiveness of the model. In addition to taking into account the frequency of individual words, this model takes into account the frequency of bigrams (pairs of words which appear together) in each of the authors' works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data by author\n",
    "byAuthor = texts.groupby(\"author\")\n",
    "\n",
    "### Tokenize (split into individual words) our text\n",
    "\n",
    "# word frequency by author\n",
    "wordFreqByAuthor = nltk.probability.ConditionalFreqDist()\n",
    "biFreqByAuthor = nltk.probability.ConditionalFreqDist()\n",
    "\n",
    "# for each author...\n",
    "for name, group in byAuthor:\n",
    "    # get all of the sentences they wrote and collapse them into a\n",
    "    # single long string\n",
    "    sentences = group['text'].str.cat(sep = ' ')\n",
    "    \n",
    "    # convert everything to lower case (so \"The\" and \"the\" get counted as \n",
    "    # the same word rather than two different words)\n",
    "    sentences = sentences.lower()\n",
    "    \n",
    "    # split the text into individual tokens    \n",
    "    tokens = nltk.tokenize.word_tokenize(sentences)\n",
    "    \n",
    "    # calculate the frequency of each token\n",
    "    frequency = nltk.FreqDist(tokens)\n",
    "    bigrams = nltk.FreqDist(nltk.bigrams(tokens))\n",
    "    # add the frequencies for each author to our dictionary\n",
    "    wordFreqByAuthor[name] = (frequency)\n",
    "    biFreqByAuthor[name] = (bigrams)\n",
    "    \n",
    "    \n",
    "# now we have an dictionary where each entry is the frequency distribution\n",
    "# of words for a specific author and another dictionary where each entry is\n",
    "# the frequency distribution of bigrams for a specific author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9177732379979571"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "\n",
    "for index, row in test.iterrows():\n",
    "    testSentence = row['text']\n",
    "    # and then lowercase & tokenize our test sentence\n",
    "    preProcessedTestSentence = nltk.tokenize.word_tokenize(testSentence.lower())\n",
    "    preProcessedBigrams = nltk.bigrams(preProcessedTestSentence)\n",
    "\n",
    "    # create an empy dataframe to put our output in\n",
    "    testProbailities = pd.DataFrame(columns = ['author','word','probability'])\n",
    "    testBiProbailities = pd.DataFrame(columns = ['author', 'bigram', 'probability'])\n",
    "\n",
    "    # For each author...\n",
    "    for i in wordFreqByAuthor.keys():\n",
    "        # for each word in our test sentence...\n",
    "        for j  in preProcessedTestSentence:\n",
    "            # find out how frequently the author used that word\n",
    "            wordFreq = wordFreqByAuthor[i].freq(j)\n",
    "            # and add a very small amount to every prob. so none of them are 0\n",
    "            smoothedWordFreq = wordFreq + 0.000001\n",
    "            # add the author, word and smoothed freq. to our dataframe\n",
    "            output = pd.DataFrame([[i, j, smoothedWordFreq]], columns = ['author','word','probability'])\n",
    "            testProbailities = testProbailities.append(output, ignore_index = True)\n",
    "\n",
    "    # For each author...\n",
    "    for i in biFreqByAuthor.keys():\n",
    "        # for each bigram in our test sentence...\n",
    "        for j  in preProcessedBigrams:\n",
    "            # find out how frequently the author used that bigram\n",
    "            biFreq = biFreqByAuthor[i].freq(j)\n",
    "            # and add a very small amount to every prob. so none of them are 0\n",
    "            smoothedBiFreq = biFreq + 0.000001\n",
    "            # add the author, word and smoothed freq. to our dataframe\n",
    "            biOutput = pd.DataFrame([[i, j, smoothedWordFreq]], columns = ['author','word','probability'])\n",
    "            testBiProbailities = testBiProbailities.append(biOutput, ignore_index = True)\n",
    "\n",
    "    # empty dataframe for the probability that each author wrote the sentence\n",
    "    testProbailitiesByAuthor = pd.DataFrame(columns = ['author','jointProbability'])\n",
    "    testBiProbailitiesByAuthor = pd.DataFrame(columns = ['author', 'jointBiProbability'])\n",
    "    \n",
    "    \n",
    "    # now let's group the dataframe with our frequency by author\n",
    "    for i in wordFreqByAuthor.keys():\n",
    "        # get the joint probability that each author wrote each word\n",
    "        oneAuthor = testProbailities.query('author == \"' + i + '\"')\n",
    "        jointProbability = oneAuthor.product(numeric_only = True)[0]\n",
    "        # and add that to our dataframe\n",
    "        output = pd.DataFrame([[i, jointProbability]], columns = ['author','jointProbability'])\n",
    "        testProbailitiesByAuthor = testProbailitiesByAuthor.append(output, ignore_index = True)\n",
    "        \n",
    "    \n",
    "    for i in biFreqByAuthor.keys():\n",
    "        oneAuthorBigram = testBiProbailities.query('author ==\" ' + i + '\"')\n",
    "       \n",
    "        jointBiProbability = oneAuthorBigram.product(numeric_only = True)[0]\n",
    "        biOutput = pd.DataFrame([[i, jointBiProbability]], columns = ['author','jointBiProbability'])\n",
    "        testBiProbailitiesByAuthor = testBiProbailitiesByAuthor.append(biOutput, ignore_index = True)\n",
    "        \n",
    "    \n",
    "    totalProbailitiesByAuthor = pd.DataFrame(testProbailitiesByAuthor.values + testBiProbailitiesByAuthor.values,\n",
    "                                            columns = testProbailitiesByAuthor.columns,\n",
    "                                            index = testProbailitiesByAuthor.index)\n",
    "        \n",
    "    # and our winner is...\n",
    "    pred = testProbailitiesByAuthor.loc[testProbailitiesByAuthor['jointProbability'].idxmax(),'author']\n",
    "    if(pred == row['author']):\n",
    "        correct = correct + 1\n",
    "        \n",
    "accuracy = correct / test.shape[0]\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factoring in the bigrams greately improved the accuracy, from 84.8% to 91.7%, an increase of almost 7 percentage points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Length\n",
    "This model builds off of the previous model, which included bigrams. On top of unigrams and bigrams, this model takes into account the frequency distribution of word lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15663, 3) (3916, 3)\n"
     ]
    }
   ],
   "source": [
    "# We will use 80% of the labeled data for training and 20% for testing\n",
    "train_size = math.floor(text_shuffle.shape[0] * .8)\n",
    "train = text_shuffle.iloc[:train_size, :]\n",
    "test = text_shuffle.iloc[train_size:, :]\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data by author\n",
    "byAuthor = texts.groupby(\"author\")\n",
    "\n",
    "### Tokenize (split into individual words) our text\n",
    "\n",
    "# word frequency by author\n",
    "wordFreqByAuthor = nltk.probability.ConditionalFreqDist()\n",
    "biFreqByAuthor = nltk.probability.ConditionalFreqDist()\n",
    "LenFreqByAuthor = nltk.probability.ConditionalFreqDist()\n",
    "\n",
    "# for each author...\n",
    "for name, group in byAuthor:\n",
    "    # get all of the sentences they wrote and collapse them into a\n",
    "    # single long string\n",
    "    sentences = group['text'].str.cat(sep = ' ')\n",
    "    \n",
    "    # convert everything to lower case (so \"The\" and \"the\" get counted as \n",
    "    # the same word rather than two different words)\n",
    "    sentences = sentences.lower()\n",
    "    \n",
    "    # split the text into individual tokens    \n",
    "    tokens = nltk.tokenize.word_tokenize(sentences)\n",
    "    \n",
    "    lengths = []\n",
    "    for i in tokens:\n",
    "        lengths.append(len(i))\n",
    "    \n",
    "    # calculate the frequency of each token\n",
    "    frequency = nltk.FreqDist(tokens)\n",
    "    bigrams = nltk.FreqDist(nltk.bigrams(tokens))\n",
    "    length = nltk.FreqDist(lengths)\n",
    "    # add the frequencies for each author to our dictionary\n",
    "    wordFreqByAuthor[name] = (frequency)\n",
    "    biFreqByAuthor[name] = (bigrams)\n",
    "    LenFreqByAuthor[name] = (length)\n",
    "\n",
    "# now we have an dictionary where each entry is the frequency distribution\n",
    "# of words for a specific author, another dictionary where each entry is\n",
    "# the frequency distribution of bigrams for a specific author, and a third\n",
    "# dictionary where each entry is the frequency distribution of the lengths\n",
    "# of words for a specific author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9177732379979571"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "\n",
    "for index, row in test.iterrows():\n",
    "    testSentence = row['text']\n",
    "    # and then lowercase & tokenize our test sentence\n",
    "    preProcessedTestSentence = nltk.tokenize.word_tokenize(testSentence.lower())\n",
    "    preProcessedBigrams = nltk.bigrams(preProcessedTestSentence)\n",
    "\n",
    "    # create an empy dataframe to put our output in\n",
    "    testProbailities = pd.DataFrame(columns = ['author','word','probability'])\n",
    "    testBiProbailities = pd.DataFrame(columns = ['author', 'bigram', 'probability'])\n",
    "    testLenProbailities = pd.DataFrame(columns = ['author', 'len', 'probability'])\n",
    "\n",
    "    # For each author...\n",
    "    for i in wordFreqByAuthor.keys():\n",
    "        # for each word in our test sentence...\n",
    "        for j  in preProcessedTestSentence:\n",
    "            # find out how frequently the author used that word\n",
    "            wordFreq = wordFreqByAuthor[i].freq(j)\n",
    "            # and add a very small amount to every prob. so none of them are 0\n",
    "            smoothedWordFreq = wordFreq + 0.000001\n",
    "            # add the author, word and smoothed freq. to our dataframe\n",
    "            output = pd.DataFrame([[i, j, smoothedWordFreq]], columns = ['author','word','probability'])\n",
    "            testProbailities = testProbailities.append(output, ignore_index = True)\n",
    "\n",
    "    # For each author...\n",
    "    for i in biFreqByAuthor.keys():\n",
    "        # for each bigram in our test sentence...\n",
    "        for j  in preProcessedBigrams:\n",
    "            # find out how frequently the author used that bigram\n",
    "            biFreq = biFreqByAuthor[i].freq(j)\n",
    "            # and add a very small amount to every prob. so none of them are 0\n",
    "            smoothedBiFreq = biFreq + 0.000001\n",
    "            # add the author, word and smoothed freq. to our dataframe\n",
    "            biOutput = pd.DataFrame([[i, j, smoothedWordFreq]], columns = ['author','word','probability'])\n",
    "            testBiProbailities = testBiProbailities.append(biOutput, ignore_index = True)\n",
    "            \n",
    "    # For each author...\n",
    "    for i in LenFreqByAuthor.keys():\n",
    "        # for each bigram in our test sentence...\n",
    "        for j  in preProcessedTestSentence:\n",
    "            jLen = len(j)\n",
    "            # find out how frequently the author used words of that length\n",
    "            lenFreq = LenFreqByAuthor[i].freq(jLen)\n",
    "            # and add a very small amount to every prob. so none of them are 0\n",
    "            smoothedBiFreq = biFreq + 0.000001\n",
    "            # add the author, word and smoothed freq. to our dataframe\n",
    "            lenOutput = pd.DataFrame([[i, jLen, smoothedWordFreq]], columns = ['author','word','probability'])\n",
    "            testLenProbailities = testLenProbailities.append(lenOutput, ignore_index = True)\n",
    "\n",
    "    # empty dataframe for the probability that each author wrote the sentence\n",
    "    testProbailitiesByAuthor = pd.DataFrame(columns = ['author','jointProbability'])\n",
    "    testBiProbailitiesByAuthor = pd.DataFrame(columns = ['author', 'jointBiProbability'])\n",
    "    testLenProbailitiesByAuthor = pd.DataFrame(columns = ['author','jointLenProbability'])\n",
    "    \n",
    "    \n",
    "    # now let's group the dataframe with our frequency by author\n",
    "    for i in wordFreqByAuthor.keys():\n",
    "        # get the joint probability that each author wrote each word\n",
    "        oneAuthor = testProbailities.query('author == \"' + i + '\"')\n",
    "        jointProbability = oneAuthor.product(numeric_only = True)[0]\n",
    "        # and add that to our dataframe\n",
    "        output = pd.DataFrame([[i, jointProbability]], columns = ['author','jointProbability'])\n",
    "        testProbailitiesByAuthor = testProbailitiesByAuthor.append(output, ignore_index = True)\n",
    "        \n",
    "    \n",
    "    for i in biFreqByAuthor.keys():\n",
    "        oneAuthorBigram = testBiProbailities.query('author ==\" ' + i + '\"')\n",
    "       \n",
    "        jointBiProbability = oneAuthorBigram.product(numeric_only = True)[0]\n",
    "        biOutput = pd.DataFrame([[i, jointBiProbability]], columns = ['author','jointBiProbability'])\n",
    "        testBiProbailitiesByAuthor = testBiProbailitiesByAuthor.append(biOutput, ignore_index = True)\n",
    "    \n",
    "    for i in LenFreqByAuthor.keys():\n",
    "        # get the joint probability that each author wrote each word\n",
    "        oneAuthorLen = testLenProbailities.query('author == \"' + i + '\"')\n",
    "        jointLenProbability = oneAuthorLen.product(numeric_only = True)[0]\n",
    "        # and add that to our dataframe\n",
    "        LenOutput = pd.DataFrame([[i, jointLenProbability]], columns = ['author','jointLenProbability'])\n",
    "        testLenProbailitiesByAuthor = testLenProbailitiesByAuthor.append(LenOutput, ignore_index = True)\n",
    "        \n",
    "\n",
    "    totalProbailitiesByAuthor = pd.DataFrame(testProbailitiesByAuthor.values + testBiProbailitiesByAuthor.values\n",
    "                                             + testLenProbailitiesByAuthor.values,\n",
    "                                            columns = testProbailitiesByAuthor.columns,\n",
    "                                            index = testProbailitiesByAuthor.index)\n",
    "        \n",
    "    # and our winner is...\n",
    "    pred = testProbailitiesByAuthor.loc[testProbailitiesByAuthor['jointProbability'].idxmax(),'author']\n",
    "    if(pred == row['author']):\n",
    "        correct = correct + 1\n",
    "        \n",
    "accuracy = correct / test.shape[0]\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facotring in the word length doesn't seem to have affected the accuracy of the model at all. Presumably these authors all tend to use words of a similar length. Alternatively, I messed something up in calculating the frequency distribution of the word lengths. Unfortunately, I'm not really sure how I can evaluate whether the lengths frequency distribution came out incorrectly.\n",
    "\n",
    "Because, for whatever reason, adding in the lengths had no effect on the model, I will use the previous model for the submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  author  jointProbability\n",
      "0    HPL      3.699657e-67\n",
      "1    EAP      4.663878e-66\n",
      "2    MWS      2.962311e-62\n",
      "  author jointProbability\n",
      "0    HPL              NaN\n",
      "1    EAP              NaN\n",
      "2    MWS              NaN\n",
      "   author jointProbability\n",
      "0  HPLHPL              NaN\n",
      "1  EAPEAP              NaN\n",
      "2  MWSMWS              NaN\n",
      "author  EAPEAP HPLHPL MWSMWS\n",
      "id                          \n",
      "id02310    NaN    NaN    NaN\n",
      "  author  jointProbability\n",
      "0    HPL     8.623553e-203\n",
      "1    EAP     2.561907e-195\n",
      "2    MWS     5.857305e-205\n",
      "  author jointProbability\n",
      "0    HPL              NaN\n",
      "1    EAP              NaN\n",
      "2    MWS              NaN\n",
      "   author jointProbability\n",
      "0  HPLHPL              NaN\n",
      "1  EAPEAP              NaN\n",
      "2  MWSMWS              NaN\n",
      "author  EAPEAP HPLHPL MWSMWS\n",
      "id                          \n",
      "id24541    NaN    NaN    NaN\n",
      "  author  jointProbability\n",
      "0    HPL     1.828107e-110\n",
      "1    EAP     3.879998e-113\n",
      "2    MWS     4.388748e-118\n",
      "  author jointProbability\n",
      "0    HPL              NaN\n",
      "1    EAP              NaN\n",
      "2    MWS              NaN\n",
      "   author jointProbability\n",
      "0  HPLHPL              NaN\n",
      "1  EAPEAP              NaN\n",
      "2  MWSMWS              NaN\n",
      "author  EAPEAP HPLHPL MWSMWS\n",
      "id                          \n",
      "id00134    NaN    NaN    NaN\n",
      "  author  jointProbability\n",
      "0    HPL     7.409993e-132\n",
      "1    EAP     1.821917e-132\n",
      "2    MWS     8.216627e-141\n",
      "  author jointProbability\n",
      "0    HPL              NaN\n",
      "1    EAP              NaN\n",
      "2    MWS              NaN\n",
      "   author jointProbability\n",
      "0  HPLHPL              NaN\n",
      "1  EAPEAP              NaN\n",
      "2  MWSMWS              NaN\n",
      "author  EAPEAP HPLHPL MWSMWS\n",
      "id                          \n",
      "id27757    NaN    NaN    NaN\n",
      "  author  jointProbability\n",
      "0    HPL      2.580925e-37\n",
      "1    EAP      2.627913e-35\n",
      "2    MWS      6.739408e-37\n",
      "  author jointProbability\n",
      "0    HPL              NaN\n",
      "1    EAP              NaN\n",
      "2    MWS              NaN\n",
      "   author jointProbability\n",
      "0  HPLHPL              NaN\n",
      "1  EAPEAP              NaN\n",
      "2  MWSMWS              NaN\n",
      "author  EAPEAP HPLHPL MWSMWS\n",
      "id                          \n",
      "id04081    NaN    NaN    NaN\n",
      "  author  jointProbability\n",
      "0    HPL     1.797730e-117\n",
      "1    EAP     2.568467e-115\n",
      "2    MWS     2.469855e-120\n",
      "  author jointProbability\n",
      "0    HPL              NaN\n",
      "1    EAP              NaN\n",
      "2    MWS              NaN\n",
      "   author jointProbability\n",
      "0  HPLHPL              NaN\n",
      "1  EAPEAP              NaN\n",
      "2  MWSMWS              NaN\n",
      "author  EAPEAP HPLHPL MWSMWS\n",
      "id                          \n",
      "id27337    NaN    NaN    NaN\n",
      "  author  jointProbability\n",
      "0    HPL      1.472842e-40\n",
      "1    EAP      4.349804e-39\n",
      "2    MWS      5.345836e-41\n",
      "  author jointProbability\n",
      "0    HPL              NaN\n",
      "1    EAP              NaN\n",
      "2    MWS              NaN\n",
      "   author jointProbability\n",
      "0  HPLHPL              NaN\n",
      "1  EAPEAP              NaN\n",
      "2  MWSMWS              NaN\n",
      "author  EAPEAP HPLHPL MWSMWS\n",
      "id                          \n",
      "id24265    NaN    NaN    NaN\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-d0cbd184cf50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# add the author, word and smoothed freq. to our dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mbiOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmoothedWordFreq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'author'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'probability'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mtestBiProbailities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestBiProbailities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbiOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# empty dataframe for the probability that each author wrote the sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv/project/MLenv/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity)\u001b[0m\n\u001b[1;32m   5169\u001b[0m             \u001b[0mto_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5170\u001b[0m         return concat(to_concat, ignore_index=ignore_index,\n\u001b[0;32m-> 5171\u001b[0;31m                       verify_integrity=verify_integrity)\n\u001b[0m\u001b[1;32m   5172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5173\u001b[0m     def join(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[0;32m~/myenv/project/MLenv/lib/python3.5/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[1;32m    211\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                        copy=copy)\n\u001b[0;32m--> 213\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv/project/MLenv/lib/python3.5/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    406\u001b[0m             new_data = concatenate_block_managers(\n\u001b[1;32m    407\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                 copy=self.copy)\n\u001b[0m\u001b[1;32m    409\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv/project/MLenv/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m   5196\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5197\u001b[0m             b = make_block(\n\u001b[0;32m-> 5198\u001b[0;31m                 \u001b[0mconcatenate_join_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5199\u001b[0m                 placement=placement)\n\u001b[1;32m   5200\u001b[0m         \u001b[0mblocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv/project/MLenv/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconcatenate_join_units\u001b[0;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[1;32m   5321\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Concatenating join units along axis0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5323\u001b[0;31m     \u001b[0mempty_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupcasted_na\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_empty_dtype_and_na\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5325\u001b[0m     to_concat = [ju.get_reindexed_values(empty_dtype=empty_dtype,\n",
      "\u001b[0;32m~/myenv/project/MLenv/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget_empty_dtype_and_na\u001b[0;34m(join_units)\u001b[0m\n\u001b[1;32m   5246\u001b[0m             \u001b[0mhas_none_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5247\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5248\u001b[0;31m             \u001b[0mdtypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5250\u001b[0m     \u001b[0mupcast_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.cache_readonly.__get__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/myenv/project/MLenv/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mdtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5539\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_filling\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5540\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5541\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5542\u001b[0m             return _get_dtype(maybe_promote(self.block.dtype,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"test.csv\")\n",
    "predictions = pd.DataFrame(columns = ['EAP', 'HPL', 'MWS', 'id'])\n",
    "for index, row in test_df.iterrows():\n",
    "    testSentence = row['text']\n",
    "    # and then lowercase & tokenize our test sentence\n",
    "    preProcessedTestSentence = nltk.tokenize.word_tokenize(testSentence.lower())\n",
    "    preProcessedBigrams = nltk.bigrams(preProcessedTestSentence)\n",
    "\n",
    "    # create an empy dataframe to put our output in\n",
    "    testProbailities = pd.DataFrame(columns = ['author','word','probability'])\n",
    "    testBiProbailities = pd.DataFrame(columns = ['author', 'bigram', 'probability'])\n",
    "\n",
    "    # For each author...\n",
    "    for i in wordFreqByAuthor.keys():\n",
    "        # for each word in our test sentence...\n",
    "        for j  in preProcessedTestSentence:\n",
    "            # find out how frequently the author used that word\n",
    "            wordFreq = wordFreqByAuthor[i].freq(j)\n",
    "            # and add a very small amount to every prob. so none of them are 0\n",
    "            smoothedWordFreq = wordFreq + 0.000001\n",
    "            # add the author, word and smoothed freq. to our dataframe\n",
    "            output = pd.DataFrame([[i, j, smoothedWordFreq]], columns = ['author','word','probability'])\n",
    "            testProbailities = testProbailities.append(output, ignore_index = True)\n",
    "\n",
    "    # For each author...\n",
    "    for i in biFreqByAuthor.keys():\n",
    "        # for each bigram in our test sentence...\n",
    "        for j  in preProcessedBigrams:\n",
    "            # find out how frequently the author used that bigram\n",
    "            biFreq = biFreqByAuthor[i].freq(j)\n",
    "            # and add a very small amount to every prob. so none of them are 0\n",
    "            smoothedBiFreq = biFreq + 0.000001\n",
    "            # add the author, word and smoothed freq. to our dataframe\n",
    "            biOutput = pd.DataFrame([[i, j, smoothedWordFreq]], columns = ['author','word','probability'])\n",
    "            testBiProbailities = testBiProbailities.append(biOutput, ignore_index = True)\n",
    "\n",
    "    # empty dataframe for the probability that each author wrote the sentence\n",
    "    testProbailitiesByAuthor = pd.DataFrame(columns = ['author','jointProbability'])\n",
    "    testBiProbailitiesByAuthor = pd.DataFrame(columns = ['author', 'jointProbability'])\n",
    "    \n",
    "    \n",
    "    # now let's group the dataframe with our frequency by author\n",
    "    for i in wordFreqByAuthor.keys():\n",
    "        # get the joint probability that each author wrote each word\n",
    "        oneAuthor = testProbailities.query('author == \"' + i + '\"')\n",
    "        jointProbability = oneAuthor.product(numeric_only = True)[0]\n",
    "        # and add that to our dataframe\n",
    "        output = pd.DataFrame([[i, jointProbability]], columns = ['author','jointProbability'])\n",
    "        testProbailitiesByAuthor = testProbailitiesByAuthor.append(output, ignore_index = True)\n",
    "        \n",
    "    print(testProbailitiesByAuthor)\n",
    "    for i in biFreqByAuthor.keys():\n",
    "        oneAuthorBigram = testBiProbailities.query('author ==\" ' + i + '\"')\n",
    "       \n",
    "        jointProbability = oneAuthorBigram.product(numeric_only = True)[0]\n",
    "        biOutput = pd.DataFrame([[i, jointProbability]], columns = ['author','jointProbability'])\n",
    "        testBiProbailitiesByAuthor = testBiProbailitiesByAuthor.append(biOutput, ignore_index = True)\n",
    "        \n",
    "    print(testBiProbailitiesByAuthor)\n",
    "    totalProbailitiesByAuthor = pd.DataFrame(testProbailitiesByAuthor.values + testBiProbailitiesByAuthor.values,\n",
    "                                            columns = testProbailitiesByAuthor.columns,\n",
    "                                            index = testProbailitiesByAuthor.index)\n",
    "    print(totalProbailitiesByAuthor)\n",
    "    totalProbailitiesByAuthor['id'] = row['id']\n",
    "    \n",
    "    \n",
    "    prob_indexed = totalProbailitiesByAuthor.pivot(index = 'id', columns='author', values='jointProbability')\n",
    "    print(prob_indexed)\n",
    "    predictions = predictions.append(prob_indexed.loc[row['id']], ignore_index = True)\n",
    "    \n",
    "predictions['id'] = test_df['id']\n",
    "predictions.to_csv(\"submission_word_freq.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My attempt to print out the results of the model with the bigrams showed there was an issue with the way it was being saved (the intermediate dataframes for some of the words have been printed up above). In hindsight, it makes sense that the authors would end up with their names duplicated, with the way I was trying to join the dataframes. I'm not sure why it's coming up with NaN values.\n",
    "\n",
    "I'm not sure how it ended up calculating a 91% accuracy rate with it turning out like that, so I suspect that my calculation was off somewhere. Below, I am exporting the results from the original model (just counting word frequency) and will submit that to the contest to see the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test.csv\")\n",
    "predictions = pd.DataFrame(columns = ['EAP', 'HPL', 'MWS', 'id'])\n",
    "for index, row in test_df.iterrows():\n",
    "    testSentence = row['text']\n",
    "    # and then lowercase & tokenize our test sentence\n",
    "    preProcessedTestSentence = nltk.tokenize.word_tokenize(testSentence.lower())\n",
    "\n",
    "    # create an empy dataframe to put our output in\n",
    "    testProbailities = pd.DataFrame(columns = ['author','word','probability'])\n",
    "\n",
    "    # For each author...\n",
    "    for i in wordFreqByAuthor.keys():\n",
    "        # for each word in our test sentence...\n",
    "        for j  in preProcessedTestSentence:\n",
    "            # find out how frequently the author used that word\n",
    "            wordFreq = wordFreqByAuthor[i].freq(j)\n",
    "            # and add a very small amount to every prob. so none of them are 0\n",
    "            smoothedWordFreq = wordFreq + 0.000001\n",
    "            # add the author, word and smoothed freq. to our dataframe\n",
    "            output = pd.DataFrame([[i, j, smoothedWordFreq]], columns = ['author','word','probability'])\n",
    "            testProbailities = testProbailities.append(output, ignore_index = True)\n",
    "\n",
    "    # empty dataframe for the probability that each author wrote the sentence\n",
    "    testProbailitiesByAuthor = pd.DataFrame(columns = ['author','jointProbability'])    \n",
    "    \n",
    "    # now let's group the dataframe with our frequency by author\n",
    "    for i in wordFreqByAuthor.keys():\n",
    "        # get the joint probability that each author wrote each word\n",
    "        oneAuthor = testProbailities.query('author == \"' + i + '\"')\n",
    "        jointProbability = oneAuthor.product(numeric_only = True)[0]\n",
    "        # and add that to our dataframe\n",
    "        output = pd.DataFrame([[i, jointProbability]], columns = ['author','jointProbability'])\n",
    "        testProbailitiesByAuthor = testProbailitiesByAuthor.append(output, ignore_index = True)\n",
    "    \n",
    "    testProbailitiesByAuthor['id'] = row['id']\n",
    "    prob_indexed = testProbailitiesByAuthor.pivot(index = 'id', columns='author', values='jointProbability')\n",
    "    predictions = predictions.append(prob_indexed.loc[row['id']], ignore_index = True)\n",
    "    \n",
    "predictions['id'] = test_df['id']\n",
    "predictions.to_csv(\"submission_word_freq.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model scored a 1.09697 in the contest, which is unsurprising considering how generally basic it is. Unfortunately, I'm running out of time/awakeness to continue working on figuring out the issue with the bigram and the word length models.\n",
    "\n",
    "The Frequency Distribution was an interesting concept, but trying to work with more than one at a time was more difficult than I expected it to be."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
